---
title: "Analysis_03_prone_incidence_allsite_eda"
author: "Chad Hochberg"
date: "`r Sys.Date()`"
output: html_document
---

#This Script is Run at a Central Coordinating Site With Access to Summary Site-level Data

#Specify Project Location and File Type
```{r}
project_location <- '/Users/chadhochberg/Library/CloudStorage/OneDrive-Johnshopkins/Research Projects/CLIF/CLIF_Projects/ProneProjects/ProningEpi/ProneSevereARF_Output_Revised'
file_type <- 'csv'

#Create Sub Folders within Project Folder
# Check if the output directory exists; if not, create it
if (!dir.exists(paste0(project_location, "/summary_analysis"))) {
  dir.create(paste0(project_location, "/summary_analysis"))
}
if (!dir.exists(paste0(project_location, "/summary_output"))) {
  dir.create(paste0(project_location, "/summary_output"))
}
if (!dir.exists(paste0(project_location, "/summary_output/graphs"))) {
  dir.create(paste0(project_location, "/summary_output/graphs"))
}
```

```{r Load Needed Libraries, include=FALSE}
packages <- c( 
              "lubridate", 
              "tidyverse", 
              "dplyr",
              "table1", 
              "broom", 
              "arrow", 
              "viridis", 
              "fst", 
              "data.table", 
              "collapse",
              "forcats",
              "MetBrewer")

install_if_missing <- function(package) {
  if (!require(package, character.only = TRUE)) {
    install.packages(package, dependencies = TRUE)
    library(package, character.only = TRUE)
  }
}

sapply(packages, install_if_missing)
rm(packages, install_if_missing)

#Use Dplyr select as default
select <- dplyr::select

```

```{r Vectors to Make File Names for Import}
#List of Sites Participating
sites <- c('hopkins', 'nu', 'ucmc', 'umn', 'rush', 'penn', 'ohsu', 'umich', 'sunnybrook')

#List of tables created by proning scripts - WITHOUT Site Name
tables <- c("_Table1_by_Hospital", 
            "_Table1_by_period",
            "_Table_Prone_Outcomes_Period",
            "_prone_per_quarter",
            "_prone_per_quarter-sarscoV2_status",
            "_prone_per_period",
            "_pfratio_sample_size",
            "_aggregate_expected_prone",
            "_aggregate_expected_prone_wCOVID-Status",
            "_aggregate_expected_prone_wCOVID-Status_post",
            "_propensity_build_estimates",
            "_propensity_build_covariance",
            "_global_aggregate_pronerisk_by_outcome",
            "_global_aggregate_pronerisk_by_outcome_period",
            "_global_aggregate_expected_prone_wCOVID-Status_post",
            "_global_aggregate_expected_prone_wCOVID-Status",
            "_adjusted_models",
            "_adjusted_models_severe_ards",
            "_adjusted_models_prone72",
            "_adjusted_models_moderate_ards",
            "_adjusted_models_ards_icd10s",
            "_prone_per_ards_severity",
            "_prone_per_period_ards_icd")

# Combine sites and tables to create full table names
site_tables <- as.vector(outer(sites, tables, paste, sep = ""))

# Print the result
site_tables
```
```{r Open Tables}

# Create an empty list to store data frames
site_table_data <- list()

# Loop through each site and each table name
for (site in sites) {
  # Construct the directory path for the current site
  site_directory <- file.path(project_location, site)  

  for (table_name in tables) {
    # Construct the full file path for the CSV
    file_path <- file.path(site_directory, paste0(site, table_name, ".", file_type))
    
    # Check if the file exists before reading
    if (file.exists(file_path)) {
      # Use a combined key of site and table name to store the data frame in the list
      site_table_data[[paste(site,table_name, sep="")]] <- fread(file_path)
    } else {
      message(paste("File not found:", file_path))
    }
  }
}

# Print the names of loaded tables
#names(site_table_data)

```
```{r Collate Data Function}
# Function to extract and collate specified tables from the site_table_data list
#This Will Only Work with Tables with the Same Number of Columns
collate_tables <- function(table_suffix) {
  # Extract only data frames that match the pattern "[SITE]_table_suffix"
  table_list <- lapply(sites, function(site) {
    # Construct the key to extract the correct data frame
    table_name <- paste(site, table_suffix, sep = "")
    
    if (table_name %in% names(site_table_data)) {
      # Add the 'site' column and return the modified data frame
      df <- site_table_data[[table_name]]
      df$site <- site
      return(df)
    } else {
      message(paste("Table not found for site:", site))
      return(NULL)  # Return NULL if the table isn't found
    }
  })
  
  # Filter out NULL elements (sites that didn't have the table)
  table_list <- Filter(Negate(is.null), table_list)
  
  # Combine all data frames into one using rbind
  combined_table <- do.call(rbind, table_list)
  
  return(combined_table)
}
```


```{r Repeat for Prone by Quarter}
# Example usage: Collate all "[SITE]_Table1_by_Hospital"
prone_per_quarter <- collate_tables("_prone_per_quarter")  |>
  select(-study_period)

#Create Overall N by Month As Well As N-proned and the like 
temp_overall_prone <- prone_per_quarter |>
  group_by(study_quarter) |>
  summarise(
    n_patients = sum(n_patients, na.rm = TRUE),
    sars_cov2_positive = sum(sars_cov2_positive, na.rm=T),
    n_proned_12_hr = sum(n_proned_12_hr, na.rm = TRUE),
    proned_12_hr_percent = n_proned_12_hr/n_patients,
    standard_error_12 = sqrt(proned_12_hr_percent * (1 - proned_12_hr_percent) / n_patients),
    n_proned_24_hr = sum(n_proned_24_hr, na.rm = TRUE),
    proned_24_hr_percent = n_proned_24_hr/n_patients,
    standard_error_24 = sqrt(proned_24_hr_percent * (1 - proned_24_hr_percent) / n_patients),
    n_proned_72_hr = sum(n_proned_72_hr, na.rm = TRUE),
    proned_72_hr_percent = n_proned_72_hr/n_patients,
    standard_error_72 = sqrt(proned_72_hr_percent * (1 - proned_72_hr_percent) / n_patients),
    n_proned_all = sum(n_proned_all, na.rm = TRUE),
    proned_percent_all = n_proned_all/n_patients,
    standard_error_all = sqrt(proned_percent_all * (1 - proned_percent_all) / n_patients),
  ) |>
  ungroup() |>
  mutate(site='all_sites')

#How Many Hospitals Overall?
n_hospitals <- prone_per_quarter |>
  group_by(site) |>
  filter(row_number()==1) |>
  ungroup() |>
  summarise(n_hospitals=sum(n_hospitals)) 

#Merge Back in With Temp File
temp_overall_prone <- temp_overall_prone |>
  mutate(n_hospitals=n_hospitals$n_hospitals)

#Now Bind Together
prone_per_quarter <- rbind(prone_per_quarter, temp_overall_prone)
```

```{r Prone Per Quarter Summarized with SARS COV2 Status}
# Example usage: Collate all "[SITE]_Table1_by_Hospital"
prone_per_quarter_covid <- collate_tables("_prone_per_quarter-sarscoV2_status")  |>
  select(-study_period)

#Create Overall N by Quarter As Well As N-proned and the like 
temp_overall_prone_covid <- prone_per_quarter_covid |>
  group_by(study_quarter, sars_cov2_positive) |>
  summarise(
    n_patients = sum(n_patients, na.rm = TRUE),
    n_proned_12_hr = sum(n_proned_12_hr, na.rm = TRUE),
    proned_12_hr_percent = n_proned_12_hr/n_patients,
    standard_error_12 = sqrt(proned_12_hr_percent * (1 - proned_12_hr_percent) / n_patients),
    n_proned_24_hr = sum(n_proned_24_hr, na.rm = TRUE),
    proned_24_hr_percent = n_proned_24_hr/n_patients,
    standard_error_24 = sqrt(proned_24_hr_percent * (1 - proned_24_hr_percent) / n_patients),
    n_proned_72_hr = sum(n_proned_72_hr, na.rm = TRUE),
    proned_72_hr_percent = n_proned_72_hr/n_patients,
    standard_error_72 = sqrt(proned_72_hr_percent * (1 - proned_72_hr_percent) / n_patients),
    n_proned_all = sum(n_proned_all, na.rm = TRUE),
    proned_percent_all = n_proned_all/n_patients,
    standard_error_all = sqrt(proned_percent_all * (1 - proned_percent_all) / n_patients),
  ) |>
  ungroup() |>
  mutate(site='all_sites')

#How Many Hospitals Overall?
n_hospitals <- prone_per_quarter_covid |>
  group_by(site) |>
  filter(row_number()==1) |>
  ungroup() |>
  summarise(n_hospitals=sum(n_hospitals)) 

#Merge Back in With Temp File
temp_overall_prone_covid <- temp_overall_prone_covid |>
  mutate(n_hospitals=n_hospitals$n_hospitals)

#Now Bind Together
prone_per_quarter_covid <- rbind(prone_per_quarter_covid, temp_overall_prone_covid)
```

```{r Create Summary Graph of Proning By Quarter}
# Reorder 'site_name' based on 'sample_size' and update pipeline
prone_per_quarter <- prone_per_quarter |>
  group_by(site) |>
  mutate(sample_size = sum(n_patients)) |>
  ungroup() |>
  mutate(site_name= fcase(
    site == 'all_sites', 'Overall',
    site == 'hopkins', 'Hopkins',
    site == 'umn', 'U Minnesota',
    site == 'penn', 'U Penn',
    site =='nu', 'Northwestern',
    site =='umich', 'U Michigan',
    site == 'ucmc', 'U Chicago',
    site == 'rush', 'Rush',
    site == 'ohsu', 'OHSU',
    site == 'sunnybrook', 'Sunnybrook'),
    site_name = fct_reorder(paste0(site_name, ", n=", sample_size), sample_size, .desc = TRUE))

# Define the range and labels for quarters
min_quarter <- min(prone_per_quarter$study_quarter)
max_quarter <- max(prone_per_quarter$study_quarter)

quarter_labels <- c(
  '2018:1-3', '', '2018:7-9', '',
  '2019:1-3', '', '2019:7-9', '',
  '2020:1-3', '', '2020:7-9', '',
  '2021:1-3', '', '2021:7-9', '',
  '2022:1-3', '', '2022:7-9', '',
  '2023:1-3', '', '2023:7-9', '',
  '2024:1-3', '', '2024:7-9', '')
study_quarter_labels <- quarter_labels[min_quarter:max_quarter]

# Create the plot with reordered legend and 'Overall' in legend
base_plot <- ggplot(prone_per_quarter[prone_per_quarter$site == 'all_sites', ],
                    aes(x = study_quarter, y = proned_12_hr_percent)) +
  geom_errorbar(aes(
    ymin = proned_12_hr_percent - (1.96 * standard_error_12), 
    ymax = proned_12_hr_percent + (1.96 * standard_error_12)), 
    width = 0.20, color = 'darkgrey', linewidth = 1.05) +
  geom_point() +
  geom_line(linewidth = 1.05, aes(color = site_name)) + # Include 'Overall' in legend
  geom_line(data = prone_per_quarter[prone_per_quarter$site != 'all_sites', ],
            aes(x = study_quarter, y = proned_12_hr_percent, 
                color = site_name), alpha = 0.2,
            linewidth = 0.90) +
  scale_color_viridis_d(option = 'H', name = 'Site') +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), 
                     labels = scales::percent, 
                     limits = c(0, 1.025),
                     name = 'Proned within 12 Hours of Enrollment') +
  scale_x_continuous(breaks = seq(min_quarter, max_quarter, by = 1),
                     labels = study_quarter_labels,
                     limits = c(min_quarter - 0.1, max_quarter + 0.1),
                     name = 'Year:Months') +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  geom_vline(xintercept = 9.6667, linetype = 2) +
  annotate("text", x = 5.667, y = 1.025, label = 'Pre COVID-19') +
  geom_vline(xintercept = 17.6667, linetype = 2) +
  annotate("text", x = 13.667, y = 1.025, label = 'COVID-19') +
  annotate("text", x = 21.667, y = 1.025, label = 'Post COVID-19')

# Display the plot
base_plot
ggsave(
  filename = 'proning_by_quarter_12_clif.pdf',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in"      # units set to inches for precise control
)

##TWO Period Graph
# Create the plot with reordered legend and 'Overall' in legend
base_plot <- ggplot(prone_per_quarter[prone_per_quarter$site == 'all_sites', ],
                    aes(x = study_quarter, y = proned_12_hr_percent)) +
  geom_errorbar(aes(
    ymin = proned_12_hr_percent - (1.96 * standard_error_12), 
    ymax = proned_12_hr_percent + (1.96 * standard_error_12)), 
    width = 0.20, color = 'darkgrey', linewidth = 1.05) +
  geom_point() +
  geom_line(linewidth = 1.05, aes(color = site_name)) + # Include 'Overall' in legend
  geom_line(data = prone_per_quarter[prone_per_quarter$site != 'all_sites', ],
            aes(x = study_quarter, y = proned_12_hr_percent, 
                color = site_name), alpha = 0.2,
            linewidth = 0.90) +
  scale_color_viridis_d(option = 'H', name = 'Site') +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), 
                     labels = scales::percent, 
                     limits = c(0, 1.025),
                     name = 'Proned within 12 Hours of Enrollment') +
  scale_x_continuous(breaks = seq(min_quarter, max_quarter, by = 1),
                     labels = study_quarter_labels,
                     limits = c(min_quarter - 0.1, max_quarter + 0.1),
                     name = 'Year:Months') +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  geom_vline(xintercept = 9.6667, linetype = 2) +
  annotate("text", x = 5.667, y = 1.025, label = 'Pre COVID-19') +
  annotate("text", x = 16.667, y = 1.025, label = 'COVID-19')
  

# Display the plot
base_plot
ggsave(
  filename = 'twoperiod_proning_by_quarter_12_clif.pdf',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in"      # units set to inches for precise control
)
```

```{r Repeat Graph but Now with Proning Within 72 Hours}
# Create the plot with reordered legend and 'Overall' in legend
base_plot <- ggplot(prone_per_quarter[prone_per_quarter$site == 'all_sites', ],
                    aes(x = study_quarter, y = proned_72_hr_percent)) +
  geom_errorbar(aes(
    ymin = proned_72_hr_percent - (1.96 * standard_error_72), 
    ymax = proned_72_hr_percent + (1.96 * standard_error_72)), 
    width = 0.20, color = 'darkgrey', linewidth = 1.05) +
  geom_point() +
  geom_line(linewidth = 1.05, aes(color = site_name)) + # Include 'Overall' in legend
  geom_line(data = prone_per_quarter[prone_per_quarter$site != 'all_sites', ],
            aes(x = study_quarter, y = proned_72_hr_percent, 
                color = site_name), alpha = 0.2,
            linewidth = 0.90) +
  scale_color_viridis_d(option = 'H', name = 'Site') +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), 
                     labels = scales::percent, 
                     limits = c(0, 1.025),
                     name = 'Proned within 72 Hours of Enrollment') +
  scale_x_continuous(breaks = seq(min_quarter, max_quarter, by = 1),
                     labels = study_quarter_labels,
                     limits = c(min_quarter - 0.1, max_quarter + 0.1),
                     name = 'Year:Months') +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  geom_vline(xintercept = 9.6667, linetype = 2) +
  annotate("text", x = 5.667, y = 1.025, label = 'Pre COVID-19') +
  geom_vline(xintercept = 17.6667, linetype = 2) +
  annotate("text", x = 13.667, y = 1.025, label = 'COVID-19') +
  annotate("text", x = 21.667, y = 1.025, label = 'Post COVID-19')

# Display the plot
base_plot
ggsave(
  filename = 'proning_by_quarter_72_clif.pdf',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in"      # units set to inches for precise control
)

##TWO PERIOD GRAPH
base_plot <- ggplot(prone_per_quarter[prone_per_quarter$site == 'all_sites', ],
                    aes(x = study_quarter, y = proned_72_hr_percent)) +
  geom_errorbar(aes(
    ymin = proned_72_hr_percent - (1.96 * standard_error_72), 
    ymax = proned_72_hr_percent + (1.96 * standard_error_72)), 
    width = 0.20, color = 'darkgrey', linewidth = 1.05) +
  geom_point() +
  geom_line(linewidth = 1.05, aes(color = site_name)) + # Include 'Overall' in legend
  geom_line(data = prone_per_quarter[prone_per_quarter$site != 'all_sites', ],
            aes(x = study_quarter, y = proned_72_hr_percent, 
                color = site_name), alpha = 0.2,
            linewidth = 0.90) +
  scale_color_viridis_d(option = 'H', name = 'Site') +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), 
                     labels = scales::percent, 
                     limits = c(0, 1.025),
                     name = 'Proned within 72 Hours of Enrollment') +
  scale_x_continuous(breaks = seq(min_quarter, max_quarter, by = 1),
                     labels = study_quarter_labels,
                     limits = c(min_quarter - 0.1, max_quarter + 0.1),
                     name = 'Year:Months') +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  geom_vline(xintercept = 9.6667, linetype = 2) +
  annotate("text", x = 5.667, y = 1.025, label = 'Pre COVID-19') +
  annotate("text", x = 16.667, y = 1.025, label = 'COVID-19')

# Display the plot
base_plot
ggsave(
  filename = 'twoperiod_proning_by_quarter_72_clif.pdf',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in"      # units set to inches for precise control
)
```
```{r Proning 12 Hours by Quarter - No Data Indicating Which Health Systems are Which}
# Separate "all_sites" and individual sites data for clarity
overall_data <- prone_per_quarter[prone_per_quarter$site == 'all_sites', ]
site_data <- prone_per_quarter[prone_per_quarter$site != 'all_sites', ]
# Dynamically create the label with sample size for the "Overall" line
overall_label <- paste("Overall Trend (n =", overall_data$sample_size[1], ")")
# Base plot
base_plot <- ggplot(overall_data, aes(x = study_quarter, y = proned_12_hr_percent)) +
  # Error bars for "all_sites" data
  geom_errorbar(
    aes(ymin = proned_12_hr_percent - (1.96 * standard_error_12), 
        ymax = proned_12_hr_percent + (1.96 * standard_error_12)), 
    width = 0.20, color = 'darkgrey', linewidth = 1.05
  ) +
  # Points for "all_sites" data
  geom_point() +
  # Main "Overall" line with dynamic label for sample size, converting label to factor
  geom_line(aes(linetype = factor(overall_label)), color = 'black', linewidth = 1.2) +
  # Define linetype legend with dynamic label
  scale_linetype_manual(values = setNames("solid", overall_label), name = "") +
  # Site-specific lines with color gradient based on sample size
  geom_line(data = site_data,
            aes(x = study_quarter, y = proned_12_hr_percent, color = sample_size, group = site),
            linewidth = 0.4, alpha = 0.4) +
  # Scale for color gradient to reflect sample size with a smaller title font size
  scale_color_gradient(
    low = "lightblue", high = "darkblue", 
    name = "Individual Health Systems\n(Line Color Represents\nNumber of Patients)"
  ) +
  # Reorder legend to display linetype legend above color gradient legend
  guides(
    linetype = guide_legend(order = 1),
    color = guide_colorbar(order = 2, title.theme = element_text(size = 9))  # Smaller font for color bar title
  ) +
  # Adjust legend font sizes in theme
  theme(
    legend.text = element_text(size = 14),   # Increase font size for linetype legend text
    legend.title = element_text(size = 14),  # Increase font size for linetype legend title
    legend.title.align = 0,  # Aligns legend title to left for clarity
  ) +
  # Y-axis for proning percentage and X-axis for study quarter with custom labels and limits
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), 
                     labels = scales::percent, 
                     limits = c(0, 1.025),
                     name = 'Proned within 12 Hours of Enrollment') +
  scale_x_continuous(breaks = seq(min_quarter, max_quarter, by = 1),
                     labels = study_quarter_labels,
                     limits = c(min_quarter - 0.1, max_quarter + 0.1),
                     name = 'Year:Months') +
  # Add COVID-19 period separator with labels
  geom_vline(xintercept = 9.6667, linetype = 2) +
  annotate("text", x = 5.667, y = 1.025, label = 'Pre COVID-19') +
  annotate("text", x = 16.667, y = 1.025, label = 'COVID-19') +
  # Classic theme and rotated x-axis labels for readability
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
# Display the plot
base_plot

ggsave(
  filename = 'twoperiod_proning_by_quarter_12_anonymous_clif.pdf',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in"      # units set to inches for precise control
)

# Separate "all_sites" and individual sites data for clarity
overall_data <- prone_per_quarter[prone_per_quarter$site == 'all_sites', ]
site_data <- prone_per_quarter[prone_per_quarter$site != 'all_sites', ]
# Dynamically create the label with sample size for the "Overall" line
overall_label <- paste("Overall Trend (n =", overall_data$sample_size[1], ")")
# Base plot
base_plot <- ggplot(overall_data, aes(x = study_quarter, y = proned_12_hr_percent)) +
  # Error bars for "all_sites" data
  geom_errorbar(
    aes(ymin = proned_12_hr_percent - (1.96 * standard_error_12), 
        ymax = proned_12_hr_percent + (1.96 * standard_error_12)), 
    width = 0.20, color = 'darkgrey', linewidth = 1.05
  ) +
  # Points for "all_sites" data
  geom_point() +
  # Main "Overall" line with dynamic label for sample size, converting label to factor
  geom_line(aes(linetype = factor(overall_label)), color = 'black', linewidth = 1.2) +
  # Define linetype legend with dynamic label
  scale_linetype_manual(values = setNames("solid", overall_label), name = "") +
  # Site-specific lines with color gradient based on sample size
  geom_line(data = site_data,
            aes(x = study_quarter, y = proned_12_hr_percent, color = sample_size, group = site),
            linewidth = 0.4, alpha = 0.4) +
  # Scale for color gradient to reflect sample size with a smaller title font size
  scale_color_gradient(
    low = "lightblue", high = "darkblue", 
    name = "Individual Health Systems\n(Line Color Represents\nNumber of Patients)"
  ) +
  # Reorder legend to display linetype legend above color gradient legend
  guides(
    linetype = guide_legend(order = 1),
    color = guide_colorbar(order = 2, title.theme = element_text(size = 9))  # Smaller font for color bar title
  ) +
  # Adjust legend font sizes in theme
  theme(
    legend.text = element_text(size = 14),   # Increase font size for linetype legend text
    legend.title = element_text(size = 14),  # Increase font size for linetype legend title
    legend.title.align = 0,  # Aligns legend title to left for clarity
  ) +
  # Y-axis for proning percentage and X-axis for study quarter with custom labels and limits
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), 
                     labels = scales::percent, 
                     limits = c(0, 1.025),
                     name = 'Proned within 12 Hours of Enrollment') +
  scale_x_continuous(breaks = seq(min_quarter, max_quarter, by = 1),
                     labels = study_quarter_labels,
                     limits = c(min_quarter - 0.1, max_quarter + 0.1),
                     name = 'Year:Months') +
  # Add COVID-19 period separator with labels
  geom_vline(xintercept = 9.6667, linetype = 2) +
  annotate("text", x = 5.667, y = 1.025, label = 'Pre COVID-19') +
  geom_vline(xintercept = 17.6667, linetype = 2) +
  annotate("text", x = 13.667, y = 1.025, label = 'COVID-19') +
  annotate("text", x = 21.667, y = 1.025, label = 'Post COVID-19') +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
# Display the plot
base_plot

ggsave(
  filename = 'proning_by_quarter_12_anonymous_clif.pdf',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in"      # units set to inches for precise control
)

ggsave(
  filename = 'proning_by_quarter_12_anonymous_clif.eps',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in",      # units set to inches for precise control
  device = cairo_ps,
  dpi = 600
)

ggsave(
  filename = 'proning_by_quarter_12_anonymous_clif.tiff',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,
  height = 6,
  units = "in",
  dpi = 600,
  compression = "lzw"
)
```

```{r Create Time Series Plot by COVID Status}
prone_per_quarter_covid <- prone_per_quarter_covid |>
  group_by(site) |>
  mutate(sample_size = sum(n_patients)) |>
  ungroup() 

all_site <- prone_per_quarter_covid[prone_per_quarter_covid$site == 'all_sites', ] |>
  mutate(study_quarter=fifelse(
    sars_cov2_positive=='COVID', study_quarter+0.02, study_quarter
  )) |>
  mutate(ymin = proned_12_hr_percent - (1.96 * standard_error_12), 
         ymax = proned_12_hr_percent + (1.96 * standard_error_12),
         ymin = fifelse(ymin<0, 0, ymin),
         ymax = fifelse(ymax>1.0, 1, ymax),
         sars_cov2_positive=fifelse(sars_cov2_positive=='Not COVID', 'No/Unknown/Pre-Pandemic', 'SARS-CoV2 Positive')) |>
  mutate(sars_cov2_positive = factor(sars_cov2_positive, levels = c('SARS-CoV2 Positive', 'No/Unknown/Pre-Pandemic')))

base_plot <- ggplot(all_site, aes(x=study_quarter, y=proned_12_hr_percent)) +
  #Error bars for "all_sites" data by COVID Status
  geom_linerange(aes(x=study_quarter, ymin = ymin, 
                    ymax = ymax, color = sars_cov2_positive), 
                width = 0.20, linewidth = 1.05, alpha=0.5) +
  geom_point(aes(color= sars_cov2_positive)) +
  geom_line(aes(group=sars_cov2_positive, color=sars_cov2_positive)) +
  # Y-axis for proning percentage and X-axis for study quarter with custom labels and limits
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), 
                     labels = scales::percent, 
                     limits = c(0, 1.025),
                     name = 'Proned within 12 Hours of Enrollment') +
  scale_x_continuous(breaks = seq(min_quarter, max_quarter, by = 1),
                     labels = study_quarter_labels,
                     limits = c(min_quarter - 0.1, max_quarter + 0.1),
                     name = 'Year:Months') +
  # Add COVID-19 period separator with labels
  geom_vline(xintercept = 9.125, linetype = 2) +
  annotate("text", x = 5.0, y = 1.025, label = 'Pre COVID-19') +
  geom_vline(xintercept = 17.6667, linetype = 2) +
  annotate("text", x = 13.667, y = 1.025, label = 'COVID-19') +
  annotate("text", x = 22.667, y = 1.025, label = 'Post COVID-19') +
  theme_classic() +
  # Adjust legend font sizes in theme
  theme(
    legend.position = c(0.14, 0.6),  # Centered horizontally and positioned at the bottom of the plot
    legend.justification = c("center", "bottom"),  # Justification for the legend
    legend.text = element_text(size = 10),   # Increase font size for linetype legend text
    legend.title = element_text(size = 10),  # Increase font size for linetype legend title
    legend.title.align = 0,  # Aligns legend title to left for clarity
  ) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  scale_color_met_d('Isfahan1', direction = -1) +
  guides(color = guide_legend(title = ''))
base_plot

ggsave(
  filename = 'proning_by_quarter_12_covid_status.pdf',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in"      # units set to inches for precise control
)

ggsave(
  filename = 'proning_by_quarter_12_covid_status.eps',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in",      # units set to inches for precise control
  device = cairo_ps,
  dpi = 600
)

ggsave(
  filename = 'proning_by_quarter_12_covid_status.tiff',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,
  height = 6,
  units = "in",
  dpi = 600,
  compression = "lzw"
)

```

```{r Period and SARS-Cov2 Figure With Post COVID SARS Pos in Quarters Given Sparse Data}
prone_summary_custom <- collate_tables("_prone_per_quarter-sarscoV2_status")  

#Collate the Non-Sars COV2 POST COVID Separately and do in 6 month periods
not_post_sars <- prone_summary_custom |>
  filter(!(study_period=='Post-COVID' & sars_cov2_positive=='COVID')) |>
  select(-study_period)
post_sars_pos <- prone_summary_custom |>
  filter(study_period=='Post-COVID' & sars_cov2_positive=='COVID') |>
  select(-study_period)

#Create Overall N by Quarter As Well As N-proned and the like 
temp_pre_post_neg <- not_post_sars |>
  group_by(study_quarter, sars_cov2_positive) |>
  summarise(
    n_patients = sum(n_patients, na.rm = TRUE),
    n_proned_12_hr = sum(n_proned_12_hr, na.rm = TRUE),
    proned_12_hr_percent = n_proned_12_hr/n_patients,
    standard_error_12 = sqrt(proned_12_hr_percent * (1 - proned_12_hr_percent) / n_patients),
    n_proned_24_hr = sum(n_proned_24_hr, na.rm = TRUE),
    proned_24_hr_percent = n_proned_24_hr/n_patients,
    standard_error_24 = sqrt(proned_24_hr_percent * (1 - proned_24_hr_percent) / n_patients),
    n_proned_72_hr = sum(n_proned_72_hr, na.rm = TRUE),
    proned_72_hr_percent = n_proned_72_hr/n_patients,
    standard_error_72 = sqrt(proned_72_hr_percent * (1 - proned_72_hr_percent) / n_patients),
    n_proned_all = sum(n_proned_all, na.rm = TRUE),
    proned_percent_all = n_proned_all/n_patients,
    standard_error_all = sqrt(proned_percent_all * (1 - proned_percent_all) / n_patients),
  ) |>
  ungroup() |>
  mutate(site='all_sites')

#Create Overall N by 6-month Period For POST-COVID SARS POS As Well As N-proned and the like 
post_sars_pos <- post_sars_pos |>
  mutate(study_quarter=ifelse(study_quarter %% 2 == 0, study_quarter, study_quarter-1)) |>
  group_by(study_quarter, sars_cov2_positive) |>
  summarise(
    n_patients = sum(n_patients, na.rm = TRUE),
    n_proned_12_hr = sum(n_proned_12_hr, na.rm = TRUE),
    proned_12_hr_percent = n_proned_12_hr/n_patients,
    standard_error_12 = sqrt(proned_12_hr_percent * (1 - proned_12_hr_percent) / n_patients),
    n_proned_24_hr = sum(n_proned_24_hr, na.rm = TRUE),
    proned_24_hr_percent = n_proned_24_hr/n_patients,
    standard_error_24 = sqrt(proned_24_hr_percent * (1 - proned_24_hr_percent) / n_patients),
    n_proned_72_hr = sum(n_proned_72_hr, na.rm = TRUE),
    proned_72_hr_percent = n_proned_72_hr/n_patients,
    standard_error_72 = sqrt(proned_72_hr_percent * (1 - proned_72_hr_percent) / n_patients),
    n_proned_all = sum(n_proned_all, na.rm = TRUE),
    proned_percent_all = n_proned_all/n_patients,
    standard_error_all = sqrt(proned_percent_all * (1 - proned_percent_all) / n_patients),
  ) |>
  ungroup() |>
  mutate(site='all_sites')

#Now Bind Together
prone_per_quarter_custom <- rbind(temp_pre_post_neg, post_sars_pos)

#Now Prepare Data Farme for ggplot
all_site <- prone_per_quarter_custom[prone_per_quarter_custom$site == 'all_sites', ] |>
  mutate(ymin = proned_12_hr_percent - (1.96 * standard_error_12), 
         ymax = proned_12_hr_percent + (1.96 * standard_error_12),
         ymin = fifelse(ymin<0, 0, ymin),
         ymax = fifelse(ymax>1.0, 1, ymax),
         sars_cov2_positive=fifelse(sars_cov2_positive=='Not COVID', 'No/Unknown/Pre-COVID', 'SARS-CoV2 Positive'))

all_site$sars_cov2_positive <- factor(all_site$sars_cov2_positive, 
                                     levels = c("SARS-CoV2 Positive", "No/Unknown/Pre-COVID"))  # Put desired order here

base_plot <- ggplot(all_site, aes(x=study_quarter, y=proned_12_hr_percent)) +
  #Error bars for "all_sites" data by COVID Status
  geom_linerange(aes(x=study_quarter, ymin = ymin, 
                    ymax = ymax, color = sars_cov2_positive), 
                width = 0.20, linewidth = 1.05, alpha=0.5) +
  geom_point(aes(color= sars_cov2_positive)) +
  geom_line(aes(group=sars_cov2_positive, color=sars_cov2_positive)) +
  # Adjust legend font sizes in theme
  theme(
    legend.text = element_text(size = 14),   # Increase font size for linetype legend text
    legend.title = element_text(size = 14),  # Increase font size for linetype legend title
    legend.title.align = 0,  # Aligns legend title to left for clarity
  ) +
  # Y-axis for proning percentage and X-axis for study quarter with custom labels and limits
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), 
                     labels = scales::percent, 
                     limits = c(0, 1.025),
                     name = 'Proned within 12 Hours of Enrollment') +
  scale_x_continuous(breaks = seq(min_quarter, max_quarter, by = 1),
                     labels = study_quarter_labels,
                     limits = c(min_quarter - 0.1, max_quarter + 0.1),
                     name = 'Year:Months') +
  # Add COVID-19 period separator with labels
  geom_vline(xintercept = 9.125, linetype = 2) +
  annotate("text", x = 5.0, y = 1.025, label = 'Pre COVID-19') +
  geom_vline(xintercept = 17.6667, linetype = 2) +
  annotate("text", x = 13.667, y = 1.025, label = 'COVID-19') +
  annotate("text", x = 22.667, y = 1.025, label = 'Post COVID-19') +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        legend.position = c(0.155, 0.75),
        legend.text = element_text(size = 8),  # Smaller legend text
        legend.key.size = unit(0.9, "lines"),  # Smaller legend keys
        legend.spacing.y = unit(0.2, "cm")) +   # Less spacing between legend items) 
  scale_color_met_d('Isfahan1', direction =-1) +
  guides(color = guide_legend(title = ''))
  base_plot
  
  ggsave(
  filename = 'proning_by_quarter_12_covid_status_6month_post.pdf',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in"      # units set to inches for precise control
)
```
```{r Proning Count Summary By ARDS Severity}
prone_summary_severity <- collate_tables("_prone_per_ards_severity")  |>
  mutate(ards_severity=fcase(
    severe_ards==1, 'severe_ards',
    severe_ards==0, 'moderate_ards'
  ))

#Create Overall N by Period and ARDS Severity
prone_summary_severity <- prone_summary_severity |>
  group_by(study_period, ards_severity) |>
  summarise(
    n_patients = sum(n_patients, na.rm = TRUE),
    n_proned_12_hr = sum(n_proned_12_hr, na.rm = TRUE),
    proned_12_hr_percent = n_proned_12_hr/n_patients,
    standard_error_12 = sqrt(proned_12_hr_percent * (1 - proned_12_hr_percent) / n_patients),
    n_proned_24_hr = sum(n_proned_24_hr, na.rm = TRUE),
    proned_24_hr_percent = n_proned_24_hr/n_patients,
    standard_error_24 = sqrt(proned_24_hr_percent * (1 - proned_24_hr_percent) / n_patients),
    n_proned_72_hr = sum(n_proned_72_hr, na.rm = TRUE),
    proned_72_hr_percent = n_proned_72_hr/n_patients,
    standard_error_72 = sqrt(proned_72_hr_percent * (1 - proned_72_hr_percent) / n_patients),
    n_proned_all = sum(n_proned_all, na.rm = TRUE),
    proned_percent_all = n_proned_all/n_patients,
    standard_error_all = sqrt(proned_percent_all * (1 - proned_percent_all) / n_patients),
  ) |>
  ungroup() |>
  mutate(site='all_sites')


#Now Prepare Data Farme for ggplot
severity_graph <- prone_summary_severity |>
  mutate(ymin = proned_12_hr_percent - (1.96 * standard_error_12), 
         ymax = proned_12_hr_percent + (1.96 * standard_error_12),
         ymin = fifelse(ymin<0, 0, ymin),
         ymax = fifelse(ymax>1.0, 1, ymax))

severity_graph$ards_severity <-
  factor(severity_graph$ards_severity, 
                  levels = c("moderate_ards", "severe_ards"))  # Put desired order here
severity_graph$study_period <-
  factor(severity_graph$study_period, 
                  levels = c("Pre-COVID", "COVID", "Post-COVID"))  # Put desired order here

base_plot <- ggplot(severity_graph, aes(x=study_period, y=proned_12_hr_percent)) +
  #Error bars for "all_sites" data by COVID Status
  geom_linerange(aes(x=study_period, ymin = ymin, 
                    ymax = ymax, color = ards_severity), 
                width = 0.20, linewidth = 1.05, alpha=0.5) +
  geom_point(aes(color= ards_severity)) +
  geom_line(aes(group=ards_severity, color=ards_severity)) +
  # Adjust legend font sizes in theme
  theme(
    legend.text = element_text(size = 14),   # Increase font size for linetype legend text
    legend.title = element_text(size = 14),  # Increase font size for linetype legend title
    legend.title.align = 0,  # Aligns legend title to left for clarity
  ) +
  # Y-axis for proning percentage and X-axis for study quarter with custom labels and limits
  scale_y_continuous(breaks = seq(0, 1, by = 0.1), 
                     labels = scales::percent, 
                     limits = c(0, 1.0),
                     name = 'Proned within 12 Hours of Enrollment') +
  theme_classic() +
  theme(legend.position = c(0.155, 0.75),
        legend.text = element_text(size = 8),  # Smaller legend text
        legend.key.size = unit(0.9, "lines"),  # Smaller legend keys
        legend.spacing.y = unit(0.2, "cm")) +   # Less spacing between legend items) 
  scale_color_met_d('Isfahan1', direction =-1) +
  guides(color = guide_legend(title = ''))
  base_plot
  
ggsave(
  filename = 'proning_by_period_severity.pdf',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in"      # units set to inches for precise control
)
```


```{r Summary for Aggregate Counts and Proning Pre and Post COVID}
#Will have to Use both Aggregate Table and STudy Quarter Table to Come To Accurate Counts
#Collate Aggregate
aggregate_data <- collate_tables('_aggregate_expected_prone')
write.csv(aggregate_data, paste0(project_location, '/summary_output/aggregate_data.csv'))

#Aggregate Data with COVID
agg_w_covid <- collate_tables('_aggregate_expected_prone_wCOVID-Status')
write.csv(agg_w_covid, paste0(project_location, '/summary_output/aggregate_data_wcovid.csv'))

prone_summary_all <- collate_tables('_prone_per_period') |>
  group_by(study_period) |>
  summarise(
      n_hospitals = sum(n_hospitals, na.rm=T),
      n_patients = sum(n_patients, na.rm = TRUE),
      across(
        starts_with("n_proned_"),
        ~ sum(.x, na.rm = TRUE),
        .names = "{.col}"
      )
    ) |>
      mutate(
        across(
          starts_with("n_proned"),
          ~ round(.x / n_patients * 100, 1),
          .names = "{.col}_percent"
        ),
        across(
        ends_with("percent"),
         ~ sqrt((.x / 100) * (1 - (.x / 100)) / n_patients),
        .names = "standard_error_{.col}"
       ),
        site='all_sites'
      )

#Rename
prone_summary_all <- prone_summary_all |>
  # Rename standard error columns
  rename_with(
    .cols = starts_with("standard_error_n"),
    .fn = ~ gsub("standard_error_n_proned_([0-9]+)_hr_percent", 
                  "standard_error_\\1", .x)
  ) |>
  # Rename percent columns
  rename_with(
    .cols = matches("^n_proned_[0-9]+_hr_percent$"),
    .fn = ~ gsub("n_proned_", "proned_", .x)
  ) |>
  rename(
   proned_percent_all =  n_proned_all_percent,
   standard_error_all = standard_error_n_proned_all_percent
  )
prone_summary <- collate_tables('_prone_per_period') |>
  rbind(prone_summary_all)
write_csv(prone_summary, paste0(project_location, '/summary_output/prone_summary_clif.csv'))
```

```{r Prone Summary by COVID}
prone_summary_covid <- collate_tables('_aggregate_expected_prone_wCOVID-Status') |>
  group_by(period_sarscov2) |>
  rename(n_proned=observed_prone) |>
  summarise(
      n_hospitals = sum(n_hospitals, na.rm=T),
      n_patients = sum(n_patients, na.rm = TRUE),
      across(
        starts_with("n_proned"),
        ~ sum(.x, na.rm = TRUE),
        .names = "{.col}"
      )
    ) |>
      mutate(
        across(
          starts_with("n_proned"),
          ~ round(.x / n_patients * 100, 1),
          .names = "{.col}_percent"
        ),
        across(
        ends_with("percent"),
         ~ sqrt((.x / 100) * (1 - (.x / 100)) / n_patients),
        .names = "standard_error_{.col}"
       ),
        site='all_sites'
      ) |>
  relocate(site, .before = 'n_hospitals')

prone_temp <- collate_tables('_aggregate_expected_prone_wCOVID-Status') |>
  group_by(site, period_sarscov2) |>
  rename(n_proned=observed_prone) |>
  summarise(
      n_hospitals = sum(n_hospitals, na.rm=T),
      n_patients = sum(n_patients, na.rm = TRUE),
      across(
        starts_with("n_proned"),
        ~ sum(.x, na.rm = TRUE),
        .names = "{.col}"
      )
    ) |>
      mutate(
        across(
          starts_with("n_proned"),
          ~ round(.x / n_patients * 100, 1),
          .names = "{.col}_percent"
        ),
        across(
        ends_with("percent"),
         ~ sqrt((.x / 100) * (1 - (.x / 100)) / n_patients),
        .names = "standard_error_{.col}"
       ))
prone_summary_covid <- rbind(prone_summary_covid, prone_temp)

write_csv(prone_summary_covid, paste0(project_location, '/summary_output/prone_summary_clif_covid.csv'))
rm(prone_temp)
```



```{r Prone Summary by COVI - Post}
prone_summary_covid <- collate_tables('_aggregate_expected_prone_wCOVID-Status_post') |>
  group_by(period_sarscov2_post) |>
  rename(n_proned=observed_prone) |>
  summarise(
      n_hospitals = sum(n_hospitals, na.rm=T),
      n_patients = sum(n_patients, na.rm = TRUE),
      across(
        starts_with("n_proned"),
        ~ sum(.x, na.rm = TRUE),
        .names = "{.col}"
      )
    ) |>
      mutate(
        across(
          starts_with("n_proned"),
          ~ round(.x / n_patients * 100, 1),
          .names = "{.col}_percent"
        ),
        across(
        ends_with("percent"),
         ~ sqrt((.x / 100) * (1 - (.x / 100)) / n_patients),
        .names = "standard_error_{.col}"
       ),
        site='all_sites'
      ) |>
  relocate(site, .before = 'n_hospitals')

prone_temp <- collate_tables('_aggregate_expected_prone_wCOVID-Status_post') |>
  group_by(site, period_sarscov2_post) |>
  rename(n_proned=observed_prone) |>
  summarise(
      n_hospitals = sum(n_hospitals, na.rm=T),
      n_patients = sum(n_patients, na.rm = TRUE),
      across(
        starts_with("n_proned"),
        ~ sum(.x, na.rm = TRUE),
        .names = "{.col}"
      )
    ) |>
      mutate(
        across(
          starts_with("n_proned"),
          ~ round(.x / n_patients * 100, 1),
          .names = "{.col}_percent"
        ),
        across(
        ends_with("percent"),
         ~ sqrt((.x / 100) * (1 - (.x / 100)) / n_patients),
        .names = "standard_error_{.col}"
       ))
prone_summary_covid <- rbind(prone_summary_covid, prone_temp)

write_csv(prone_summary_covid, paste0(project_location, '/summary_output/prone_summary_clif_covid_post.csv'))
rm(prone_temp)
```


```{r Repeat Prone Summary by Severity}
prone_summary_severity <- collate_tables("_prone_per_ards_severity")  |>
  mutate(ards_severity=fcase(
    severe_ards==1, 'severe_ards',
    severe_ards==0, 'moderate_ards'
  )) |>
  group_by(study_period, ards_severity) |>
  summarise(
      n_hospitals = sum(n_hospitals, na.rm=T),
      n_patients = sum(n_patients, na.rm = TRUE),
      across(
        starts_with("n_proned_"),
        ~ sum(.x, na.rm = TRUE),
        .names = "{.col}"
      )
    ) |>
      mutate(
        across(
          starts_with("n_proned"),
          ~ round(.x / n_patients * 100, 1),
          .names = "{.col}_percent"
        ),
        across(
        ends_with("percent"),
         ~ sqrt((.x / 100) * (1 - (.x / 100)) / n_patients),
        .names = "standard_error_{.col}"
       ),
        site='all_sites'
      )

#Rename
prone_summary_temp <- prone_summary_severity |>
  # Rename standard error columns
  rename_with(
    .cols = starts_with("standard_error_n"),
    .fn = ~ gsub("standard_error_n_proned_([0-9]+)_hr_percent", 
                  "standard_error_\\1", .x)
  ) |>
  # Rename percent columns
  rename_with(
    .cols = matches("^n_proned_[0-9]+_hr_percent$"),
    .fn = ~ gsub("n_proned_", "proned_", .x)
  ) |>
  rename(
   proned_percent_all =  n_proned_all_percent,
   standard_error_all = standard_error_n_proned_all_percent
  )
prone_summary_severity <- collate_tables('_prone_per_ards_severity') |>
  mutate(ards_severity=fcase(
    severe_ards==1, 'severe_ards',
    severe_ards==0, 'moderate_ards'
  )) |>
  select(-severe_ards) |>
  rbind(prone_summary_temp)
write_csv(prone_summary_severity , paste0(project_location, '/summary_output/prone_summary_by_severity.csv'))
rm(prone_summary_temp)
```

```{r Repeat Prone Summary by ARDS ICD-10}
prone_summary_icd <- collate_tables("_prone_per_period_ards_icd")  |>
  group_by(study_period, ards_icd_10) |>
  summarise(
      n_hospitals = sum(n_hospitals, na.rm=T),
      n_patients = sum(n_patients, na.rm = TRUE),
      across(
        starts_with("n_proned_"),
        ~ sum(.x, na.rm = TRUE),
        .names = "{.col}"
      )
    ) |>
      mutate(
        across(
          starts_with("n_proned"),
          ~ round(.x / n_patients * 100, 1),
          .names = "{.col}_percent"
        ),
        across(
        ends_with("percent"),
         ~ sqrt((.x / 100) * (1 - (.x / 100)) / n_patients),
        .names = "standard_error_{.col}"
       ),
        site='all_sites'
      )

#Rename
prone_summary_temp <- prone_summary_icd |>
  # Rename standard error columns
  rename_with(
    .cols = starts_with("standard_error_n"),
    .fn = ~ gsub("standard_error_n_proned_([0-9]+)_hr_percent", 
                  "standard_error_\\1", .x)
  ) |>
  # Rename percent columns
  rename_with(
    .cols = matches("^n_proned_[0-9]+_hr_percent$"),
    .fn = ~ gsub("n_proned_", "proned_", .x)
  ) |>
  rename(
   proned_percent_all =  n_proned_all_percent,
   standard_error_all = standard_error_n_proned_all_percent
  )
prone_summary_icd <- collate_tables("_prone_per_period_ards_icd")|>
  rbind(prone_summary_temp)
write_csv(prone_summary_icd , paste0(project_location, '/summary_output/prone_summary_ards_icd10.csv'))
rm(prone_summary_temp)
```


```{r Bar Chart of Admits and COVID Status Over Time}
base_plot <- ggplot(subset(prone_per_quarter_covid, site =='all_sites'), 
                    aes(x=study_quarter, y=n_patients, fill = factor(sars_cov2_positive))) +
  geom_col() +
   scale_fill_manual(
    values = c("Not COVID" = "#40005C", "COVID" = "#FF8B10"),
    labels = c("Not COVID" = "COVID Negative/Unknown or \n  Pre-COVID", 
               "COVID" = "SARS-CoV2 Positive"),
    name = ""
  ) +
  scale_y_continuous(name = 'Number of Admissions',
                     limits = c(0, 510)) +
  scale_x_continuous(breaks = seq(min_quarter, max_quarter, by = 1),
                     labels = study_quarter_labels,
                     #limits = c(0, max_quarter + 0.1),
                     name = 'Year:Months') +
  # Add COVID-19 period separator with labels
  geom_vline(xintercept = 8.60, linetype = 2) +
  annotate("text", x = 5.0, y = 505, label = 'Pre COVID-19', size=3.3) +
  geom_vline(xintercept = 17.5, linetype = 2) +
  annotate("text", x = 14.0, y = 505, label = 'COVID-19', size=3.3) +
  annotate("text", x = 21.667, y = 505, label = 'Post COVID-19', size=3.3) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
     legend.position = c(0.155, 0.75),
        legend.text = element_text(size = 8),  # Smaller legend text
        legend.key.size = unit(0.9, "lines"),  # Smaller legend keys
        legend.spacing.y = unit(0.2, "cm"))    # Less spacing between legend items

# Display the plot
base_plot

ggsave(
  filename = 'admits_over_time.pdf',
  plot = base_plot,
  path = paste0(project_location, '/summary_output/graphs/'),
  width = 12,       # adjust width to make it wide
  height = 6,       # adjust height for PowerPoint-friendly proportions
  units = "in"      # units set to inches for precise control
)
```


```{r Collate Table 1}
# Function to extract and collate specified tables from the site_table_data list
#This Will Only Work with Tables with the Same Number of Columns
# Function to extract and collate specified tables from the site_table_data list
collate_table_one <- function(table_suffix) {
  # Extract only data frames that match the pattern "[SITE]_table_suffix"
  table_list <- lapply(sites, function(site) {
    # Construct the key to extract the correct data frame
    table_name <- paste(site, table_suffix, sep = "")
    
    if (table_name %in% names(site_table_data)) {
      # Retrieve the data frame and add the 'site' column
      df <- site_table_data[[table_name]]
      colnames(df) <- as.character(unlist(df[1, ]))
      df <- df |>
        filter(row_number()>1)
      df <- df |> select(characteristic, site, Overall, percent_missing)  # Select necessary columns
      return(df)
    } else {
      message(paste("Table not found for site:", site))
      return(NULL)  # Return NULL if the table isn't found
    }
  })
  
  # Filter out NULL elements (sites that didn't have the table)
  table_list <- Filter(Negate(is.null), table_list)
  
  # Combine all data frames into one using rbind
  combined_table <- do.call(rbind, table_list)
  
  return(combined_table)
}

#Function to Clean Characteristic Colum
clean_characteristic <- function(x) {
  x %>% 
    str_trim() %>%              # remove leading/trailing spaces
    str_remove("^X\\.+")        # remove X followed by one-or-more dots
}

# Run the function
tableone_master <- collate_table_one("_Table1_by_Hospital") |>
   mutate(characteristic = clean_characteristic(characteristic))

#Remove All Variables Between Hospital and Year
# Function to remove elements between 'hospital_id....' and next 'year....'
vars <- tableone_master$characteristic 
remove_between_hospital_year <- function(x) {
  start_idx <- which(grepl("hospital_id", x))
  end_idx <- which(grepl("year", x))
  
  # Create vector of indices to remove
  to_remove <- c()
  for (s in start_idx) {
    e <- end_idx[end_idx >= s][1]
    if (!is.na(e)) {
      to_remove <- c(to_remove, s:e)
    }
  }
  return(x[-to_remove])
}

# Apply function
clean_vars <- remove_between_hospital_year(vars)

##Filter Table ONe Master to These Clean Variable Lists
tableone_master <- tableone_master |>
  filter(characteristic %in% clean_vars)

#FOr Sites with Only Two STudy Periods Need to Correct Period Numbers for Summation
new_data <- data.frame(
  characteristic = rep('COVID', 2),
  site = c('rush', 'sunnybrook'),
  Overall = c('157 (71.6)', '75 (50.0)'),
  percent_missing = rep("", 2)
)

tableone_master <- rbind(tableone_master, new_data) |>
  mutate(characteristic=fifelse(
    site=='Sunnybrook' & characteristic =='race_ethnicity = unknown (%)', 'unknown', characteristic)
  )
```



```{r Table 1 Wide - Second}
#These are Cleaner Labels for the 'Characteristics' Variable
unique_vars <- unique(clean_vars)
unique_vars

wide_table_scaffold <- c(
    "n",
    "age_at_admission (mean (SD))",
    "female",
    "Race_ethnicity",
    "Asian",
    "Black, non-Hispanic",
    "Hispanic",
    "other_race",
    "unknown_race",
    "White, non-Hispanic",
    "bmi (mean (SD))",
    "2018",
    "2019",
    "2020",
    "2021",
    "2022",
    "2023",
    "2024",
    "Study Period",
    "COVID",
    "Post_COVID",
    "Pre_COVID",
    "Period and SARS-CoV2",
    "COVIDPeriod_SARCoV2Neg.Unk",
    "COVID_SarsCov2Pos",
    "Post_COVID_SarsCov2Neg.Unk",
    "Post_COVID_SarsCov2Pos",
    "Pre_COVID_Testing",
    "eligible_by_proseva",
    "eligible_by_prone",
    "admit_to_enrolled (mean (SD)",
    "ett_to_enrolled (mean (SD))",
    "or_before_enrollment",
    "min_pf_ratio (mean (SD))",
    "severe_ards",
    "first_vent_mode",
    "AssistControl-VolumeControl",
    "Other_Vent_Mode",
    "Pressure_Control",
    "Pressure_Support/CPAP",
    "Pressure-Regulated-Volume-Control",
    "SIMV",
    "Volume Support",
    "first_proseva_peep (mean (SD))",
    "first_proseva_fio2 (mean (SD))",
    "sofa_score (mean (SD))",
    "Norepi Equivalent",
    "No Norepi",
    "LessThan0.01",
    "MoreThan0.10",
    "APRV",
    "age_at_admission (mean (SD))",
    "female",
    "Race_ethnicity",
    "Black, non-Hispanic",
    "White, non-Hispanic",
    "bmi (mean (SD))",
    "Post_COVID",
    "Period and SARS-CoV2",
    "COVIDPeriod_SARCoV2Neg.Unk",
    "Post_COVID_SarsCov2Neg.Unk",
    "Post_COVID_SarsCov2Pos",
    "eligible_by_proseva",
    "eligible_by_prone",
    "admit_to_enrolled (mean (SD)",
    "ett_to_enrolled (mean (SD))",
    "or_before_enrollment",
    "min_pf_ratio (mean (SD))",
    "severe_ards",
    "first_vent_mode",
    "AssistControl-VolumeControl",
    "Pressure-Regulated-Volume-Control",
    "Pressure_Control",
    "Pressure_Support/CPAP",
    "first_proseva_peep (mean (SD))",
    "first_proseva_fio2 (mean (SD))",
    "sofa_score (mean (SD))",
    "Norepi Equivalent",
    "White, non-Hispanic",
    "AssistControl-VolumeControl",
    "Other_Vent_Mode",
    "Pressure_Control",
    "Pressure_Support/CPAP",
    "SIMV",
    "unknown_race",
    "Study Period",
    "Pre_COVID",
    "Pressure_Control"
    )

# Create lookup vector
label_lookup <- setNames(wide_table_scaffold, unique_vars)

# Now map labels to all of clean_vars
mapped_labels <- label_lookup[clean_vars]

# Result
mapped_labels
tableone_master$characteristic <- mapped_labels[tableone_master$characteristic]
```

```{r Work Towards an Overall Table}
table1_work <- tableone_master |>
  rename(site_data=Overall) |>
  # Extract mean and SD using regex
  mutate(site_data=trimws(site_data),
         site_mean_count=as.numeric(sub("\\s*\\(.*", "", site_data)), # Gets the mean/count
         site_sd_perc=as.numeric(sub(".*\\((.*)\\).*", "\\1", site_data))) |>
  #Define a Column is a Mean or SD Column
  mutate(mean_col=fifelse(
            grepl('mean', characteristic, ignore.case=T), 1, 0))|>
  group_by(characteristic, mean_col) |>
  mutate(count=fifelse(
         mean_col==0, sum(site_mean_count, na.rm=T), NaN)
  ) |>
  #Calculate Proportion Each Site Contributes (for Weighted Averages)
  group_by(site) |>
  mutate(site_n=fifelse(characteristic=='n', site_mean_count, NaN),
         site_n=sum(site_n, na.rm=T),
         #Account for Missingness
         percent_missing=as.numeric(percent_missing),
         percent_missing=fifelse(is.na(percent_missing), 0, percent_missing),
         value_n = round((1 - percent_missing /100) * site_n, 0), site_n) |>
  ungroup() |>
  group_by(characteristic) |>
  mutate(overall_n=sum(value_n, na.rm = TRUE),
        weights = value_n / overall_n,
        percent = fifelse(mean_col==0, round((count/overall_n)*100, 2), NaN)) |>
   ungroup() |>
  #Calculate Weighted Means - Checked and This Matches HMisc Function (wtd.mean)
  group_by(characteristic) |>
  mutate(weighted_mean=fifelse(
    mean_col==1, round(sum(site_mean_count*weights, na.rm=T), 3), NaN)) |>
  mutate(
  pooled_sd = fifelse(
    mean_col == 1,
    round(sqrt(sum((value_n - 1) * site_sd_perc^2, na.rm = T) / 
                 sum(value_n - 1, na.rm = T)), 2), NaN)) |>
  ungroup()

table1 <- table1_work |>
  filter(site=='hopkins') |>
  mutate(site='Overall')
```

```{r P Values For Between Site Comparisons Using Weighted ANOVA/Chi2 -2}
#Identify Columns For Weighted ANOVA (continuous Vars)
mean_cols <- table1$characteristic[table1$mean_col==1]

#Function for Weighted ANOVA from Summary Data
#
weighted_anova_pval <- function(df, cols) {
  df <- df |>
    group_by(characteristic) |>
    filter(characteristic %in% cols) |>
  # Total number of groups
    mutate(k=length(site_mean_count)) |>
    #Between-group sum of squares (SSB)
    mutate(ss_between=sum(value_n * ((site_mean_count-weighted_mean)^2), na.rm=T),
           ss_within=sum((value_n - 1) * (site_sd_perc^2), na.rm=T),
           # Degrees of freedom
           df_between = k-1,
           df_within = overall_n-k, 
           # Mean squares
           ms_between = ss_between/df_between,
           ms_within = ss_within/df_within,
           f_stat = round(ms_between/ms_within, 4),
           p_value = round(pf(f_stat, df_between, df_within, lower.tail = FALSE), 4))
  
  new_columns <- df |>
    select(characteristic, f_stat, p_value) |>
    filter(row_number()==1) |>
    ungroup()
  return(new_columns)
}
group_p_values <- weighted_anova_pval(table1_work, mean_cols)

#Now for Catgorical Weighted Chi 2
#Have to Do Separately for Single and Multi Category Variables
single_cat_cols <- table1$characteristic[table1$mean_col==0 &
                                         table1$characteristic %in% c('female',
                                      'eligible_by_proseva',
                                      'eligible_by_prone',
                                      'severe_ards')]

weighted_chisq <- function(df, cols, add_to) {
  #Filter to Categorical Columns
  df_chi <- df |>
    filter(characteristic %in% cols) |>
    group_by(characteristic) |>
    mutate(expected = (percent / 100) * site_n) |>
    summarise(
      chisq = sum(weights * (site_mean_count - expected)^2 / expected, na.rm = TRUE),
      dfree = n() - 1,
      p_value = round(pchisq(chisq, df = dfree, lower.tail = FALSE), 4),
      .groups = "drop"
  )
  
  df_chi <- df_chi |>
    select(characteristic, chisq, p_value)
  
  add_to <- full_join(add_to, df_chi)
  return(add_to)
}
group_p_values <- weighted_chisq(table1_work, single_cat_cols, group_p_values)

#Have to Take a Different Approach to MultiVariable Categories
weighted_chisq_multicat <- function(df, cat_var, parent_name, add_to) {
  ## Step 1: Filter to Categories Within Parent Cateogry
  sub_df <- df %>%
  filter(characteristic %in% cat_var) %>%
  #Calculate Expected
  mutate(expected = (percent / 100) * site_n)

chisq <- sum(sub_df$weights * (sub_df$site_mean_count - sub_df$expected)^2 / sub_df$expected, na.rm = TRUE)
n_categories <- length(unique(sub_df$characteristic))
n_sites <- length(unique(sub_df$site))
dfree <- (n_categories - 1) * (n_sites - 1)

dfree <- length(unique(sub_df$characteristic)) * (n_sites - 1)  # or just (n_categories - 1) * (n_sites - 1)

final_df <- data.frame(
  characteristic = parent_name,
  chisq = round(chisq, 4),
  p_value = round(pchisq(chisq, df = dfree, lower.tail = FALSE), 4)
)

add_to <- full_join(add_to, final_df)
  return(add_to)
}
#Now for Race/Ethnicity Categories
race_cat <- c('Asian', 'Black, non-Hispanic', 'Hispanic', 'other_race', 'unknown_race', 'White, non-Hispanic')
group_p_values <- weighted_chisq_multicat(table1_work, race_cat, parent_name='Race_ethnicity', group_p_values)
#Now for Year
years <- c('2018', '2019', '2020', '2021', '2022', '2023', '2024')
group_p_values <- weighted_chisq_multicat(table1_work, years, parent_name='2018', group_p_values)
study_period <- c('Pre_COVID', 'COVID', 'Post_COVID')
group_p_values <- weighted_chisq_multicat(table1_work, study_period, parent_name='Study Period', group_p_values)
sars_period <- c('COVIDPeriod_SARCoV2Neg.Unk', 
                 'COVID_SarsCov2Pos', 
                 'Post_COVID_SarsCov2Neg.Unk', 
                 'Post_COVID_SarsCov2Pos')
group_p_values <- weighted_chisq_multicat(table1_work, sars_period, parent_name='Period and SARS-CoV2', group_p_values)

vent_mode <- c('AssistControl-VolumeControl', 'Other_Vent_Mode', 'Pressure_Control',
               'Pressure_Support/CPAP', 'Pressure-Regulated-Volume-Control', 'SIMV')
group_p_values <- weighted_chisq_multicat(table1_work, vent_mode, parent_name='first_vent_mode', group_p_values)

norepi <- c('No Norepi', 'LessThan0.01', 'MoreThan0.10')
group_p_values <- weighted_chisq_multicat(table1_work, norepi, parent_name='Norepi Equivalent', group_p_values)
```

```{r Put Table One Together -2}
table1_collated <- table1_work |>
  filter(characteristic!='Year') |> #Workaround for having an extra 'Year' column in Sunnybrook Data
  rbind(table1) |>
  mutate(value_n=fifelse(site=='Overall', overall_n, value_n),
         site_mean_count=fifelse(site=='Overall' & mean_col==1, weighted_mean, site_mean_count),
         site_mean_count=fifelse(site=='Overall' & mean_col==0, count, site_mean_count),
         site_sd_perc=fifelse(site=='Overall' & mean_col==1, pooled_sd, site_sd_perc),
         site_sd_perc=fifelse(site=='Overall' & mean_col==0, percent, site_sd_perc)) |>
  pivot_wider(
    id_cols = c('characteristic', 'mean_col'),
    names_from = site,
    values_from = c('site_mean_count', 'site_sd_perc', 'value_n'),
    names_glue = "{site}_{.value}"
  ) |>
  select(-mean_col) |>
  left_join(group_p_values) |>
  relocate(
    c('Overall_value_n' , 'Overall_site_mean_count', 'Overall_site_sd_perc'), .after = characteristic
  )
```

```{r Formatted Table One -2}
format_table1_for_reporting <- function(df) {
  # Infer mean_col from characteristic if not already present
  if (!"mean_col" %in% names(df)) {
    df <- df %>%
      mutate(mean_col = ifelse(grepl("mean \\(SD\\)", characteristic), 1, 0))
  }

  # Get site names
  site_names <- unique(gsub("_site_mean_count", "", grep("_site_mean_count", names(df), value = TRUE)))

  # Start building output
  formatted_table <- df %>%
    select(characteristic, mean_col)

  for (site in site_names) {
    mean_col_name <- paste0(site, "_site_mean_count")
    sd_col_name   <- paste0(site, "_site_sd_perc")
    n_col_name    <- paste0(site, "_value_n")

    if (!(mean_col_name %in% names(df)) | !(sd_col_name %in% names(df)) | !(n_col_name %in% names(df))) {
      next
    }

    formatted_col <- mapply(function(mean, sd_or_pct, n, is_mean, var_name) {
      if (is.na(mean) || is.na(sd_or_pct) || is.na(n)) {
        return(NA_character_)
      }

      # Special case: raw "n" row  just count
      if (var_name == "n") {
        return(as.character(round(mean)))
      }

      # Continuous variable
      if (is_mean == 1) {
        sprintf("%.2f (%.2f)", round(mean, 2), round(sd_or_pct, 2))
      } else {
        pct <- round((mean / n) * 100, 1)
        sprintf("%d (%.1f%%)", round(mean), pct)
      }
    },
    mean = df[[mean_col_name]],
    sd_or_pct = df[[sd_col_name]],
    n = df[[n_col_name]],
    is_mean = df$mean_col,
    var_name = df$characteristic,
    SIMPLIFY = TRUE
    )

    formatted_table[[site]] <- formatted_col
  }

  return(formatted_table)
}
table1_formatted <- format_table1_for_reporting(table1_collated) |>
  left_join(group_p_values) |>
  select(-mean_col) |>
  mutate(value_n=table1_collated$Overall_value_n) |>
  relocate(value_n, .after = 'characteristic') |>
  relocate(p_value, .before = 'f_stat')
write.csv(table1_formatted, paste0(project_location, '/summary_output/table1_formatted.csv'))
```

```{r Load Estimates and Covariance Metrics for Each Site - Calculate Global Estimates for Fixed Effects}
load_est <- function(site_name) {
  est_key <- paste0(site_name, "_propensity_build_estimates")
  df <- site_table_data[[est_key]]
  as.numeric(df$estimate)
}

load_cov <- function(site_name) {
  cov_key <- paste0(site_name, "_propensity_build_covariance")
  df <- site_table_data[[cov_key]]
  # Extract row names from V1 and remove the column
  rownames(df) <- df$V1
  df$V1 <- NULL
  data.matrix(df)
}


fixed_eff_est <- lapply(sites, load_est)
fixed_eff_cov <- lapply(sites, load_cov)

# Compute the global estimate
fixed_eff_prec <- lapply(fixed_eff_cov, function(cov_mtrx) solve(cov_mtrx))

## Global precision (inverse covariance) is just the sum
global_eff_prec <- Reduce("+", fixed_eff_prec)
global_eff_cov <- solve(global_eff_prec)

## Global coef is the weighted average
global_eff_est <- rep(0, nrow(global_eff_cov))
for (i in 1:length(sites)) {
  global_eff_est <- global_eff_est + fixed_eff_prec[[i]] %*% fixed_eff_est[[i]]
}
global_eff_est <- global_eff_cov %*% global_eff_est

eff_df <- data.frame(
  'variable' = rownames(global_eff_est),
  'coefficient' = global_eff_est
) |>
  pivot_wider(names_from = 'variable', values_from = 'coefficient', names_prefix='coef_')

#Save Global Coefficients
write.csv(eff_df, paste0(project_location, '/summary_output/global_coefficients.csv'))
```

```{r Table1 by Period}
# Function to extract and collate specified tables from the site_table_data list
#This Will Only Work with Tables with the Same Number of Columns
# Function to extract and collate specified tables from the site_table_data list
collate_table_one_period <- function(table_suffix) {
  # Extract only data frames that match the pattern "[SITE]_table_suffix"
  table_list <- lapply(sites, function(site) {
    # Construct the key to extract the correct data frame
    table_name <- paste(site, table_suffix, sep = "")
    
    if (table_name %in% names(site_table_data)) {
      # Retrieve the data frame and add the 'site' column
      df <- site_table_data[[table_name]]
      colnames(df) <- as.character(unlist(df[1, ]))
      df <- df |>
        filter(row_number()>1)
      # Define the desired columns
    desired_cols <- c("characteristic", "site", 
                      "Pre-COVID", "COVID", "Post-COVID", "percent_missing")

# Make sure these columns exist
missing_cols <- setdiff(desired_cols, names(df))
if (length(missing_cols) > 0) {
  for (col in missing_cols) {
    df[[col]] <- NA  # Add missing columns as NA
  }
}

# Reorder columns
df <- df |> select(all_of(desired_cols))
      return(df)
    } else {
      message(paste("Table not found for site:", site))
      return(NULL)  # Return NULL if the table isn't found
    }
  })
  
  # Filter out NULL elements (sites that didn't have the table)
  table_list <- Filter(Negate(is.null), table_list)
  
  # Combine all data frames into one using rbind
  combined_table <- do.call(rbind, table_list)
  
  return(combined_table)
}

# Run the function
tableone_master_period <- collate_table_one_period("_Table1_by_period") |>
   mutate(characteristic = clean_characteristic(characteristic))

#Remove All Variables Between Hospital and Year
# Function to remove elements between 'hospital_id....' and next 'year....'
vars <- tableone_master_period $characteristic
# Apply function
clean_vars <- remove_between_hospital_year(vars)

##Filter Table ONe Master to These Clean Variable Lists
tableone_master_period <- tableone_master_period |>
  filter(characteristic %in% clean_vars) |>
  mutate(characteristic=fifelse(
    site=='Sunnybrook' & characteristic =='race_ethnicity = unknown (%)', 'unknown', characteristic)
  )
```

```{r Table 1 Wide}
#These are Cleaner Labels for the 'Characteristics' Variable
unique_vars <- unique(clean_vars)

wide_table_scaffold <- c(
    "n",
    "age_at_admission (mean (SD))",
    "female",
    "Race_ethnicity",
    "Asian",
    "Black, non-Hispanic",
    "Hispanic",
    "other_race",
    "unknown_race",
    "White, non-Hispanic",
    "bmi (mean (SD))",
    "2018",
    "2019",
    "2020",
    "2021",
    "2022",
    "2023",
    "2024",
    "Study Period",
    "COVID",
    "Post_COVID",
    "Pre_COVID",
    "Period and SARS-CoV2",
    "COVIDPeriod_SARCoV2Neg.Unk",
    "COVID_SarsCov2Pos",
    "Post_COVID_SarsCov2Neg.Unk",
    "Post_COVID_SarsCov2Pos",
    "Pre_COVID_Testing",
    "eligible_by_proseva",
    "eligible_by_prone",
    "admit_to_enrolled (mean (SD)",
    "ett_to_enrolled (mean (SD))",
    "or_before_enrollment",
    "min_pf_ratio (mean (SD))",
    "severe_ards",
    "first_vent_mode",
    "AssistControl-VolumeControl",
    "Other_Vent_Mode",
    "Pressure_Control",
    "Pressure_Support/CPAP",
    "Pressure-Regulated-Volume-Control",
    "SIMV",
    "Volume Support",
    "first_proseva_peep (mean (SD))",
    "first_proseva_fio2 (mean (SD))",
    "sofa_score (mean (SD))",
    "Norepi Equivalent",
    "No Norepi",
    "LessThan0.01",
    "MoreThan0.10",
    "APRV",
    "age_at_admission (mean (SD))",
    "female",
    "Race_ethnicity",
    "Black, non-Hispanic",
    "White, non-Hispanic",
    "bmi (mean (SD))",
    "Post_COVID",
    "Period and SARS-CoV2",
    "COVIDPeriod_SARCoV2Neg.Unk",
    "Post_COVID_SarsCov2Neg.Unk",
    "Post_COVID_SarsCov2Pos",
    "eligible_by_proseva",
    "eligible_by_prone",
    "admit_to_enrolled (mean (SD)",
    "ett_to_enrolled (mean (SD))",
    "or_before_enrollment",
    "min_pf_ratio (mean (SD))",
    "severe_ards",
    "first_vent_mode",
    "AssistControl-VolumeControl",
    "Pressure-Regulated-Volume-Control",
    "Pressure_Control",
    "Pressure_Support/CPAP",
    "first_proseva_peep (mean (SD))",
    "first_proseva_fio2 (mean (SD))",
    "sofa_score (mean (SD))",
    "Norepi Equivalent",
    "White, non-Hispanic",
    "AssistControl-VolumeControl",
    "Other_Vent_Mode",
    "Pressure_Control",
    "Pressure_Support/CPAP",
    "SIMV",
    "unknown_race",
    "Study Period",
    "Pre-COVID",
    "Pressure_Control"
    )

# Create lookup vector
label_lookup <- setNames(wide_table_scaffold, unique_vars)

# Now map labels to all of clean_vars
mapped_labels <- label_lookup[clean_vars]

# Result
tableone_master_period$characteristic <- mapped_labels[tableone_master_period$characteristic] 

#Go to Long FOrm
table1_work_period <- tableone_master_period |>
  filter(characteristic!='year') |>
  pivot_longer(cols = c(`Pre-COVID`, COVID, `Post-COVID`),
               names_to = 'period')
```

```{r Work Towards an Overall Table - 2}
table1_work_period <- table1_work_period |>
  # Extract mean and SD using regex
  mutate(value=trimws(value),
         site_mean_count=as.numeric(sub("\\s*\\(.*", "", value)), # Gets the mean/count
         site_sd_perc=as.numeric(sub(".*\\((.*)\\).*", "\\1", value))) |>
  #Define a Column is a Mean or SD Column
  mutate(mean_col=fifelse(
            grepl('mean', characteristic, ignore.case=T), 1, 0)) |>
  group_by(characteristic, period, mean_col) |>
  mutate(count=fifelse(
         mean_col==0, sum(site_mean_count, na.rm=T), NaN)
  ) |>
  #Calculate Proportion Each Site Contributes (for Weighted Averages)
  group_by(site, period) |>
  mutate(site_n=fifelse(characteristic=='n', site_mean_count, NaN),
         site_n=sum(site_n, na.rm=T),
         #Account for Missingness
         percent_missing=as.numeric(percent_missing),
         percent_missing=fifelse(is.na(percent_missing), 0, percent_missing),
         value_n = round((1 - percent_missing /100) * site_n, 0), site_n) |>
  ungroup() |>
  group_by(characteristic, period) |>
  mutate(overall_n=sum(value_n, na.rm = TRUE),
        weights = value_n / overall_n,
        percent = fifelse(mean_col==0, round((count/overall_n)*100, 2), NaN)) |>
   ungroup() |>
  #Calculate Weighted Means by Period and Site - Checked and This Matches HMisc Function (wtd.mean)
  group_by(characteristic, period) |>
  mutate(weighted_mean=fifelse(
    mean_col==1, round(sum(site_mean_count*weights, na.rm=T), 3), NaN)) |>
  mutate(
   pooled_sd = fifelse(
     mean_col == 1,
     round(sqrt(sum((value_n - 1) * site_sd_perc^2, na.rm = T) / 
                 sum(value_n - 1, na.rm = T)), 2), NaN)) |>
  ungroup()

table1_period <- table1_work_period |>
  filter(site=='hopkins') |>
  mutate(site='Overall')
```

```{r P Values For Between Site Comparisons Using Weighted ANOVA/Chi2}
#Identify Columns For Weighted ANOVA (continuous Vars)
mean_cols <- table1_work_period$characteristic[table1_work_period$mean_col==1]

#Function for Weighted ANOVA from Summary Data
weighted_anova_pval <- function(df, cols) {
  df <- df |>
    group_by(characteristic, period) |>
    filter(characteristic %in% cols) |>
  # Total number of groups
    mutate(k=length(site_mean_count)) |>
    #Between-group sum of squares (SSB)
    mutate(ss_between=sum(value_n * ((site_mean_count-weighted_mean)^2), na.rm=T),
           ss_within=sum((value_n - 1) * (site_sd_perc^2), na.rm=T),
           # Degrees of freedom
           df_between = k-1,
           df_within = overall_n-k, 
           # Mean squares
           ms_between = ss_between/df_between,
           ms_within = ss_within/df_within,
           f_stat = round(ms_between/ms_within, 4),
           p_value = round(pf(f_stat, df_between, df_within, lower.tail = FALSE), 4))
  
  new_columns <- df |>
    select(characteristic, period, f_stat, p_value) |>
    filter(row_number()==1) |>
    ungroup()
  return(new_columns)
}
group_p_values <- weighted_anova_pval(table1_work_period, mean_cols)

#Now for Catgorical Weighted Chi 2
#Have to Do Separately for Single and Multi Category Variables
single_cat_cols <- table1_period$characteristic[table1_period$mean_col==0 &
                                         table1_period$characteristic %in% c('female',
                                      'eligible_by_proseva',
                                      'eligible_by_prone',
                                      'severe_ards')]

weighted_chisq <- function(df, cols, add_to) {
  #Filter to Categorical Columns
  df_chi <- df |>
    filter(characteristic %in% cols) |>
    group_by(characteristic, period) |>
    mutate(expected = (percent / 100) * site_n) |>
    summarise(
      chisq = sum(weights * (site_mean_count - expected)^2 / expected, na.rm = TRUE),
      dfree = n() - 1,
      p_value = round(pchisq(chisq, df = dfree, lower.tail = FALSE), 4),
      .groups = "drop"
  )
  
  df_chi <- df_chi |>
    select(characteristic, period, chisq, p_value)
  
  add_to <- full_join(add_to, df_chi)
  return(add_to)
}
group_p_values <- weighted_chisq(table1_work_period, single_cat_cols, group_p_values)

#Have to Take a Different Approach to MultiVariable Categories
weighted_chisq_multicat <- function(df, cat_var, parent_name, add_to) {
  ## Step 1: Filter to Categories Within Parent Cateogry
  sub_df <- df %>%
  filter(characteristic %in% cat_var) %>%
  #Calculate Expected
  group_by(period) |>
  mutate(expected = (percent / 100) * site_n) |>
    summarise(
    chisq = sum(weights * (site_mean_count - expected)^2 / expected, na.rm = TRUE),
    n_categories = n_distinct(characteristic),
    n_sites = n_distinct(site),
    dfree = (n_categories - 1) * (n_sites - 1),
    .groups = "drop"  # Ungroup after summarising
  )

#Pull Out as Vectors
chisq <- sub_df$chisq
dfree <- sub_df$dfree 

final_df <- data.frame(
  characteristic = parent_name,
  period = sub_df$period,
  chisq = round(chisq, 4),
  p_value = round(pchisq(chisq, df = dfree, lower.tail = FALSE), 4)
)

add_to <- full_join(add_to, final_df)
  return(add_to)
}
#Now for Race/Ethnicity Categories
race_cat <- c('Asian', 'Black, non-Hispanic', 'Hispanic', 'other_race', 'unknown_race', 'White, non-Hispanic')
group_p_values <- weighted_chisq_multicat(table1_work_period, race_cat, parent_name='Race_ethnicity', group_p_values)
#Now for Year
years <- c('2018', '2019', '2020', '2021', '2022', '2023', '2024')
group_p_values <- weighted_chisq_multicat(table1_work_period, years, parent_name='2018', group_p_values)
study_period <- c('Pre_COVID', 'COVID', 'Post_COVID')
group_p_values <- weighted_chisq_multicat(table1_work_period, study_period, parent_name='Study Period', group_p_values)
sars_period <- c('COVIDPeriod_SARCoV2Neg.Unk', 
                 'COVID_SarsCov2Pos', 
                 'Post_COVID_SarsCov2Neg.Unk', 
                 'Post_COVID_SarsCov2Pos')
group_p_values <- weighted_chisq_multicat(table1_work_period, sars_period, parent_name='Period and SARS-CoV2', group_p_values)

vent_mode <- c('AssistControl-VolumeControl', 'Other_Vent_Mode', 'Pressure_Control',
               'Pressure_Support/CPAP', 'Pressure-Regulated-Volume-Control', 'SIMV')
group_p_values <- weighted_chisq_multicat(table1_work_period, vent_mode, parent_name='first_vent_mode', group_p_values)

norepi <- c('No Norepi', 'LessThan0.01', 'MoreThan0.10')
group_p_values <- weighted_chisq_multicat(table1_work_period, norepi, parent_name='Norepi Equivalent', group_p_values) 
```
```{r Now get P Values for Comparison Across Periods Rather THan Site}
#Identify Columns For Weighted ANOVA (continuous Vars)
mean_cols <- table1_work_period$characteristic[table1_work_period$mean_col==1]

#Function for Weighted ANOVA from Summary Data
weighted_anova_pval <- function(df, cols) {
  df <- df |>
    group_by(characteristic) |>
    filter(characteristic %in% cols) |>
  # Total number of groups
    mutate(k=length(site_mean_count)) |>
    #Between-group sum of squares (SSB)
    #Here Use the Weighted Mean per Period that is Already Calculated
    mutate(ss_between=sum(overall_n * ((weighted_mean-mean(weighted_mean, na.rm=T))^2), na.rm=T),
           ss_within=sum((overall_n - 1) * (site_sd_perc^2), na.rm=T),
           # Degrees of freedom
           df_between = k-1,
           df_within = sum(overall_n, na.rm=T)-k, 
           # Mean squares
           ms_between = ss_between/df_between,
           ms_within = ss_within/df_within,
           period_f_stat = round(ms_between/ms_within, 4),
           period_p_value = round(pf(period_f_stat, df_between, df_within, lower.tail = FALSE), 4))
  
  new_columns <- df |>
    select(characteristic, period, period_f_stat, period_p_value) |>
    filter(row_number()==1) |>
    ungroup()
  return(new_columns)
}
period_p_values <- weighted_anova_pval(table1_period, mean_cols)

#Now for Catgorical Weighted Chi 2
#Have to Do Separately for Single and Multi Category Variables
single_cat_cols <- table1_period$characteristic[table1_period$mean_col==0 &
                                         table1_period$characteristic %in% c('female',
                                      'eligible_by_proseva',
                                      'eligible_by_prone',
                                      'severe_ards')]

weighted_chisq <- function(df, cols, add_to) {
  #Filter to Categorical Columns
  df_chi <- df |>
    filter(characteristic %in% cols) |>
    group_by(characteristic) |>
    mutate(expected = (sum(count, na.rm=T) /sum(overall_n, na.rm=T))*overall_n) |>
    summarise(
      period_chisq = sum((count - expected)^2 / expected,  #   test statistic
                 na.rm = TRUE),
      dfree = n() - 1,
      period_p_value = round(pchisq(period_chisq, df = dfree, lower.tail = FALSE), 4),
      .groups = "drop"
  ) |>
    mutate(period = 'Pre-COVID')
  
  df_chi <- df_chi |>
    select(characteristic, period, period_chisq, period_p_value)
  
  add_to <- full_join(add_to, df_chi)
  return(add_to)
}
period_p_values <- weighted_chisq(table1_period, single_cat_cols, period_p_values)

#Have to Take a Different Approach to MultiVariable Categories
weighted_chisq_multicat <- function(df, cat_var, parent_name, add_to) {
  ## Step 1: Filter to Categories Within Parent Cateogry
  sub_df <- df %>%
  filter(characteristic %in% cat_var) %>%
  group_by(characteristic) %>%
  #Calculate Expected
  mutate(expected=(sum(count, na.rm=T) /sum(overall_n, na.rm=T))*overall_n) |>
  ungroup() %>%
  summarise(
    period_chisq = sum((count - expected)^2 / expected,  #   test statistic
                 na.rm = TRUE),
    n_categories = n_distinct(characteristic),
    n_periods = n_distinct(period),
    dfree = (n_categories - 1) * (n_periods - 1),
    .groups = "drop"  # Ungroup after summarising
  )

#Pull Out as Vectors
period_chisq <- sub_df$period_chisq
dfree <- sub_df$dfree 

final_df <- data.frame(
  characteristic = parent_name,
  period = 'Pre-COVID',
  period_chisq = round(period_chisq, 4),
  period_p_value = round(pchisq(period_chisq, df = dfree, lower.tail = FALSE), 4)
)

add_to <- full_join(add_to, final_df)
  return(add_to)
}
#Now for Race/Ethnicity Categories
race_cat <- c('Asian', 'Black, non-Hispanic', 'Hispanic', 'other_race', 'unknown_race', 'White, non-Hispanic')
period_p_values <- weighted_chisq_multicat(table1_period, race_cat, parent_name='Race_ethnicity', period_p_values)
#Now for Year
years <- c('2018', '2019', '2020', '2021', '2022', '2023', '2024')
period_p_values <- weighted_chisq_multicat(table1_period, years, parent_name='2018', period_p_values)
study_period <- c('Pre_COVID', 'COVID', 'Post_COVID')
period_p_values <- weighted_chisq_multicat(table1_period, study_period, parent_name='Study Period', period_p_values)
sars_period <- c('COVIDPeriod_SARCoV2Neg.Unk', 
                 'COVID_SarsCov2Pos', 
                 'Post_COVID_SarsCov2Neg.Unk', 
                 'Post_COVID_SarsCov2Pos')
period_p_values <- weighted_chisq_multicat(table1_period, sars_period, parent_name='Period and SARS-CoV2', period_p_values)

vent_mode <- c('AssistControl-VolumeControl', 'Other_Vent_Mode', 'Pressure_Control',
               'Pressure_Support/CPAP', 'Pressure-Regulated-Volume-Control', 'SIMV')
period_p_values <- weighted_chisq_multicat(table1_period, vent_mode, parent_name='first_vent_mode', period_p_values)

norepi <- c('No Norepi', 'LessThan0.01', 'MoreThan0.10')
period_p_values <- weighted_chisq_multicat(table1_period, norepi, parent_name='Norepi Equivalent', period_p_values)
```



```{r Put Table One Period Together}
table1_collated <- table1_work_period |>
  rbind(table1_period) |>
  group_by(period) |>
  mutate(value_n=fifelse(site=='Overall', overall_n, value_n),
         site_mean_count=fifelse(site=='Overall' & mean_col==1, weighted_mean, site_mean_count),
         site_mean_count=fifelse(site=='Overall' & mean_col==0, count, site_mean_count),
         site_sd_perc=fifelse(site=='Overall' & mean_col==1, pooled_sd, site_sd_perc),
         site_sd_perc=fifelse(site=='Overall' & mean_col==0, percent, site_sd_perc)) |>
  select(-mean_col) |>
  pivot_wider(
    id_cols = c('characteristic'),
    names_from = c('site', 'period'),
    values_from = c('site_mean_count', 'site_sd_perc', 'value_n'),
    names_glue = "{site}_{period}_{.value}"
  ) |>
  left_join(period_p_values) |>
  left_join(group_p_values) |>
  relocate(
    dplyr::starts_with('overall'), .after = characteristic
  ) |>
  group_by(characteristic) |>
  slice_head(n=1) |>
  ungroup()
```

```{r Formatted Table One By Period}
format_table1_for_reporting <- function(df) {
  # Infer mean_col from characteristic if not already present
  if (!"mean_col" %in% names(df)) {
    df <- df %>%
      mutate(mean_col = ifelse(grepl("mean \\(SD\\)", characteristic), 1, 0))
  }

  # Get site names
  site_names <- unique(gsub("_site_mean_count", "", grep("_site_mean_count", names(df), value = TRUE)))

  # Start building output
  formatted_table <- df %>%
    select(characteristic, mean_col)

  for (site in site_names) {
    mean_col_name <- paste0(site, "_site_mean_count")
    sd_col_name   <- paste0(site, "_site_sd_perc")
    n_col_name    <- paste0(site, "_value_n")

    if (!(mean_col_name %in% names(df)) | !(sd_col_name %in% names(df)) | !(n_col_name %in% names(df))) {
      next
    }

    formatted_col <- mapply(function(mean, sd_or_pct, n, is_mean, var_name) {
      if (is.na(mean) || is.na(sd_or_pct) || is.na(n)) {
        return(NA_character_)
      }

      # Special case: raw "n" row  just count
      if (var_name == "n") {
        return(as.character(round(mean)))
      }

      # Continuous variable
      if (is_mean == 1) {
        sprintf("%.2f (%.2f)", round(mean, 2), round(sd_or_pct, 2))
      } else {
        pct <- round((mean / n) * 100, 1)
        sprintf("%d (%.1f%%)", round(mean), pct)
      }
    },
    mean = df[[mean_col_name]],
    sd_or_pct = df[[sd_col_name]],
    n = df[[n_col_name]],
    is_mean = df$mean_col,
    var_name = df$characteristic,
    SIMPLIFY = TRUE
    )

    formatted_table[[site]] <- formatted_col
  }

  return(formatted_table)
}
table1_formatted_period <- format_table1_for_reporting(table1_collated) |>
  left_join(period_p_values) |>
  left_join(group_p_values) |>
  group_by(characteristic) |>
  slice_head(n=1) |>
  select(-mean_col) |>
  relocate(p_value, .before = 'f_stat') |>
  relocate(period_p_value, period_f_stat, period_chisq, .after = 'Overall_Post-COVID')

#Same Order as Original Table
table1_formatted_period <- table1_formatted |> select(characteristic) |>
  left_join(table1_formatted_period)
write.csv(table1_formatted_period, paste0(project_location, '/summary_output/table1_byperiod_formatted.csv'))
```

```{r Collate Global Aggregate Tables}
global_aggregate_by_proned <- collate_tables('_global_aggregate_pronerisk_by_outcome')
write.csv(global_aggregate_by_proned, paste0(project_location, '/summary_output/global_aggregate_byoutcome.csv'))

global_aggregate_by_proned_period <- collate_tables('_global_aggregate_pronerisk_by_outcome_period')
write.csv(global_aggregate_by_proned_period, paste0(project_location, '/summary_output/global_aggregate_byoutcome_period.csv'))

global_aggregate_by_proned_period_sars <- collate_tables('_global_aggregate_expected_prone_wCOVID-Status')
write.csv(global_aggregate_by_proned_period_sars, paste0(project_location, '/summary_output/global_aggregate_by_proned_period_sars.csv'))

global_aggregate_by_proned_period_sars <- collate_tables('_global_aggregate_expected_prone_wCOVID-Status_post')
write.csv(global_aggregate_by_proned_period_sars, paste0(project_location, '/summary_output/global_aggregate_by_proned_period_sars_post.csv'))
```

```{r Proning Practice Aggregate Table}
#Collate Prone Outcome Per Period
prone_practices <- collate_table_one_period("_Table_Prone_Outcomes_Period")  |>
    mutate(characteristic = clean_characteristic(characteristic)) |>
    filter(characteristic %in% 
     c(
      'n',
      'proned = 1 (%)',
      'prone_episodes (mean (SD))',
      'first_prone_episode_hours (mean (SD))',
      'median_pt_prone_duration (mean (SD))',
      'mean_pt_prone_duration (mean (SD))',
      'time_to_prone_hrs (mean (SD))'
    ))

#Go to Long FOrm
table_prone_work_period <- prone_practices |>
  pivot_longer(cols = c(`Pre-COVID`, COVID, `Post-COVID`),
               names_to = 'period')

table_prone_work_period <- table_prone_work_period |>
  # Extract mean and SD using regex
  mutate(value=trimws(value),
         site_mean_count=as.numeric(sub("\\s*\\(.*", "", value)), # Gets the mean/count
         site_sd_perc=as.numeric(sub(".*\\((.*)\\).*", "\\1", value))) |>
  #Define a Column is a Mean or SD Column
  mutate(mean_col=fifelse(
            grepl('mean', characteristic, ignore.case=T), 1, 0)) |>
  group_by(characteristic, period, mean_col) |>
  mutate(count=fifelse(
         mean_col==0, sum(site_mean_count, na.rm=T), NaN)
  ) |>
  #Calculate Proportion Each Site Contributes (for Weighted Averages)
  group_by(site, period) |>
  mutate(site_n=fifelse(characteristic=='proned = 1 (%)', site_mean_count, NaN),
         site_n=sum(site_n, na.rm=T),
         #Account for Missingness
         percent_missing=as.numeric(percent_missing),
         percent_missing=fifelse(is.na(percent_missing), 0, percent_missing),
         value_n = site_n) |>
  ungroup() |>
  group_by(characteristic, period) |>
  mutate(overall_n=sum(value_n, na.rm = TRUE),
        weights = value_n / overall_n,
        percent = fifelse(mean_col==0, round((count/overall_n)*100, 2), NaN)) |>
   ungroup() |>
  #Calculate Weighted Means by Period and Site - Checked and This Matches HMisc Function (wtd.mean)
  group_by(characteristic, period) |>
  mutate(weighted_mean=fifelse(
    mean_col==1, round(sum(site_mean_count*weights, na.rm=T), 3), NaN)) |>
  mutate(
   pooled_sd = fifelse(
     mean_col == 1,
     round(sqrt(sum((value_n - 1) * site_sd_perc^2, na.rm = T) / 
                 sum(value_n - 1, na.rm = T)), 2), NaN)) |>
  ungroup()

table_prone_practice_period <- table_prone_work_period  |>
  filter(site=='hopkins') |>
  mutate(site='Overall')
```

```{r Finalize Proning Practice Table}
#Use Weighted ANOVA Function
#Identify Columns For Weighted ANOVA (continuous Vars)
mean_cols <- table_prone_practice_period$characteristic[table_prone_practice_period$mean_col==1]

period_p_values <- weighted_anova_pval(table_prone_practice_period, mean_cols)

table_prone_collated <- table_prone_practice_period |>
  rbind(table_prone_work_period) |>
  group_by(period) |>
  mutate(value_n=fifelse(site=='Overall', overall_n, value_n),
         site_mean_count=fifelse(site=='Overall' & mean_col==1, weighted_mean, site_mean_count),
         site_mean_count=fifelse(site=='Overall' & mean_col==0, count, site_mean_count),
         site_sd_perc=fifelse(site=='Overall' & mean_col==1, pooled_sd, site_sd_perc),
         site_sd_perc=fifelse(site=='Overall' & mean_col==0, percent, site_sd_perc)) |>
  select(-mean_col) |>
  pivot_wider(
    id_cols = c('characteristic'),
    names_from = c('site', 'period'),
    values_from = c('site_mean_count', 'site_sd_perc', 'value_n'),
    names_glue = "{site}_{period}_{.value}"
  ) |>
  left_join(period_p_values) |>
  relocate(
    dplyr::starts_with('overall'), .after = characteristic
  ) |>
  group_by(characteristic) |>
  slice_head(n=1) |>
  ungroup()

table_prone_formatted_period <- format_table1_for_reporting(table_prone_collated) |>
  left_join(period_p_values) |>
  group_by(characteristic) |>
  slice_head(n=1) |>
  select(-mean_col) |>
  relocate(period_p_value, period_f_stat, .after = 'Overall_Post-COVID')

#Same Order as Original Table
final_prone_practice <- prone_practices |> 
  filter(site=='hopkins') |>
  select(characteristic) |> 
  left_join(table_prone_formatted_period)

write.csv(final_prone_practice, paste0(project_location, '/summary_output/prone_practices_continuous.csv'))
```

```{r Prepare Data for Meta-analysis}
primary_analysis <- collate_tables('_adjusted_models') |>
  filter(term %in%
           c('study_periodPost-COVID',
             'study_periodPre-COVID',
             'period_sarscov2COVID_SarsCov2Pos',
             'period_sarscov2Post-COVID_SarsCov2Neg-Unk',
             'period_sarscov2Post-COVID_SarsCov2Pos',
             'period_sarscov2Pre-COVID',
             'period_sarscov2_postCOVID_SarsCov2Pos',
             'period_sarscov2_postPost-COVID',
             'period_sarscov2_postPre-COVID'
             )) |>
  group_by(site, term) |>
  arrange(site, V1) |>
  slice_head(n=1) |>
  arrange(site, V1) |>
  ungroup() |>
  mutate(analysis='primary')

prone72 <- collate_tables('_adjusted_models_prone72') |>
  filter(term %in%
           c('study_periodPost-COVID',
             'study_periodPre-COVID',
             'period_sarscov2COVID_SarsCov2Pos',
             'period_sarscov2Post-COVID_SarsCov2Neg-Unk',
             'period_sarscov2Post-COVID_SarsCov2Pos',
             'period_sarscov2Pre-COVID',
             'period_sarscov2_postCOVID_SarsCov2Pos',
             'period_sarscov2_postPost-COVID',
             'period_sarscov2_postPre-COVID'
             )) |>
  group_by(site, term) |>
  arrange(site, V1) |>
  slice_head(n=1) |>
  arrange(site, V1) |>
  ungroup() |>
  mutate(analysis='prone72')

prone_severe <- collate_tables('_adjusted_models_severe_ards') |>
  filter(term %in%
           c('study_periodPost-COVID',
             'study_periodPre-COVID',
             'period_sarscov2COVID_SarsCov2Pos',
             'period_sarscov2Post-COVID_SarsCov2Neg-Unk',
             'period_sarscov2Post-COVID_SarsCov2Pos',
             'period_sarscov2Pre-COVID',
             'period_sarscov2_postCOVID_SarsCov2Pos',
             'period_sarscov2_postPost-COVID',
             'period_sarscov2_postPre-COVID'
             )) |>
  group_by(site, term) |>
  arrange(site, V1) |>
  slice_head(n=1) |>
  arrange(site, V1) |>
  ungroup() |>
  mutate(analysis='severe_ards')

prone_moderate <- collate_tables('_adjusted_models_moderate_ards') |>
  filter(term %in%
           c('study_periodPost-COVID',
             'study_periodPre-COVID',
             'period_sarscov2COVID_SarsCov2Pos',
             'period_sarscov2Post-COVID_SarsCov2Neg-Unk',
             'period_sarscov2Post-COVID_SarsCov2Pos',
             'period_sarscov2Pre-COVID',
             'period_sarscov2_postCOVID_SarsCov2Pos',
             'period_sarscov2_postPost-COVID',
             'period_sarscov2_postPre-COVID'
             )) |>
  group_by(site, term) |>
  arrange(site, V1) |>
  slice_head(n=1) |>
  arrange(site, V1) |>
  ungroup() |>
  mutate(analysis='moderate_ards')

prone_ards_icd10 <- collate_tables('_adjusted_models_ards_icd10s') |>
  filter(term %in%
           c('study_periodPost-COVID',
             'study_periodPre-COVID',
             'period_sarscov2COVID_SarsCov2Pos',
             'period_sarscov2Post-COVID_SarsCov2Neg-Unk',
             'period_sarscov2Post-COVID_SarsCov2Pos',
             'period_sarscov2Pre-COVID',
             'period_sarscov2_postCOVID_SarsCov2Pos',
             'period_sarscov2_postPost-COVID',
             'period_sarscov2_postPre-COVID'
             )) |>
  group_by(site, term) |>
  arrange(site, V1) |>
  slice_head(n=1) |>
  arrange(site, V1) |>
  ungroup() |>
  mutate(analysis='ards_icd10')

model_terms_meta <- rbind(primary_analysis, prone72, prone_severe, prone_moderate, prone_ards_icd10) |>
  arrange(site, analysis, V1) |>
  #Remove Where Estimates Could Not be Generated
  filter(!upper_bound=='Inf')
write_csv(model_terms_meta, paste0(project_location, '/summary_output/model_terms_meta.csv'))
```


```{r Prepare to Load Tables Into List}
#List of tables without site name to expext
tables <- c(
  "_cat_outcomes_by_period.csv",
  "_cont_outcomes_by_period.csv",
  "_cat_outcomes_by_proned_period.csv",
  "_cont_outcomes_by_proned_period.csv",
  "_vent_summary_by_period_infusions.csv",
  "_vent_summary_by_proned_period_infusion.csv",
  "_vent_summary_by_period.csv",
  "_vent_summary_by_proned_period.csv"
)

# Combine sites and tables to create full table names
site_tables <- as.vector(outer(sites, tables, paste, sep = ""))
```

```{r Open Tables for CSV and RDS}
# Create an empty list to store data frames
site_table_data <- list()

# Loop through each site and each table name
for (site in sites) {
  # Construct the directory path for the current site
  site_directory <- file.path(paste0(project_location,'/',site))  

  for (table_name in tables) {
    # Construct the full file path for the CSV
    file_path <- file.path(site_directory, paste0(site, table_name))
    
    # Check if the file exists before reading
    if (file.exists(file_path)) {
      # Use a combined key of site and table name to store the data frame in the list
      site_table_data[[paste(site,table_name, sep="")]] <- fread(file_path)
    } else {
      message(paste("File not found:", file_path))
    }
  }
}


combine_lists <- function(data_list, table_name) {
  # Create a placeholder for combined data
  combined_table <- do.call(rbind, lapply(names(data_list), function(name) {
    x <- data_list[[name]]  # Access the list element by name
    if (!is.null(x[[table_name]])) {
      # Extract the data frame
      df <- x[[table_name]]
      
      # Extract the site name from the list name
      site_name <- strsplit(name, "_")[[1]][1]  # Get the part before the underscore
      
      # Create a new column for the site
      df$site <- site_name
      
      return(df)
    } else {
      return(NULL)
    }
  }))
  
  return(combined_table)
}
```

```{r Summary Data For Ventilator Practice by Period}
#For Purposes of this Study Will Use Exposure metric (PF defined vs SF Defined)

table_vent_period <- collate_tables("_vent_summary_by_period.csv") #Continuous Outcomes
table_infusion_period  <- collate_tables("_vent_summary_by_period_infusions.csv") #Categorical/Binary Outcomes

#Determine Total N for Patients Represented, Total CLIF Sites and Total Hospitals
N_aggregate <- table_infusion_period |> 
  group_by(site, strata) |>
  summarise(
    n_site=max(n)
  ) |>
  group_by(strata) |>
  mutate(
    N_aggregate=sum(n_site, na.rm=T),
    n_sites=n_distinct(site)) |>
  ungroup() |>
  arrange(site, strata)

#In This Case Any Missing Categorical Data is Considered that Patient Did Not Get Drug (after quality control). Use aggregate counts from 'N_aggregate' to get n's; For example some sites never used inhaled pulmonaryvasodilators in this cohort
#Now Aggregate Categorical Variables
aggregate_infusion <- table_infusion_period |>
  group_by(strata, variable, level) |>
  dplyr::reframe(
    n_total=sum(n),
    n_miss=sum(miss, na.rm=T),
    count=sum(freq,na.rm=T),
    percent=count/(n_total-n_miss)
  ) |>
  #This Step Only Necessary When Some Categories Are Not in Some Sites Data (and that is the true underlying structure)
  left_join(N_aggregate %>% select(strata, N_aggregate) %>% distinct, by=c('strata')) |>
  mutate(
    n_total=N_aggregate,
    percent=count/(n_total-n_miss)
  ) |>
  select(-N_aggregate)

#Order of Variables
order_vec <- table_infusion_period$variable
order <- data.frame(
  variable=unique(order_vec)) |>
  mutate(order_number=dplyr::row_number())

aggregate_infusion <- aggregate_infusion |>
  left_join(order, by='variable') |>
  arrange(strata, order_number)
```


```{r Test For Difference in Distribution of Categorical Variables}
#For now Table 1 Purposes This Ignores Clustering by Site
vars <- unique(aggregate_infusion$variable) #Can do this as All Variables are Unique

chisq_function <- function(cat_table) {
  chi_df <- data.frame(
    variable = character(),
    chisq = numeric(),
    p.value = numeric()
  )
  
  for (v in vars) {
    x <- cat_table |>
      filter(variable == v)
    
    # Check if there are any rows for the variable
    if (nrow(x) > 0) {
      contingency_table <- xtabs(count ~ level + strata, data = x)
      
      # Print the contingency table to check
      cat('\n')
      print(prop.table(contingency_table, margin=2))
      
      # Perform the chi-squared test
      #First Need to Remove Overall Strata if it Exists
      x <- x |> filter(strata!='Overall')
      contingency_table <- xtabs(count ~ level + strata, data = x)
      chi <- chisq.test(contingency_table)
      # Append the results to chi_df
      chi_df <- rbind(
        chi_df,
        data.frame(
          variable = v,
          chisq = as.numeric(chi$statistic),
          p.value = as.numeric(round(chi$p.value, digits = 4))
        )
      )
    } else {
      # If no data for the variable, append NA values
      cat('\nNo Data for', v)
    }
  }
  return(chi_df)  # Return the results data frame
}

tab_chi <- chisq_function(aggregate_infusion) |>
  mutate(rn=1)

#Merge Back with Table 1 and Convert to Wide
table_infusion_paper <- aggregate_infusion |>
  select(strata, variable, level, n_total, count, percent, order_number) |>
  mutate(
    percent=round((percent*100), digits = 2)
  ) |>
  rename(count_mean=count,
         percent_sd=percent) |>
  pivot_wider(
    names_from=c(strata),
    names_glue="{strata}_{.value}",
    values_from=c(n_total, count_mean, percent_sd)) 

#Vector For Reordering Columns
strata_values <- c('Overall', 'Pre-COVID', 'COVID', 'Post-COVID')
relocate_vector <- c()
# Loop through the strata values to generate names for those strata
for (stratum in strata_values) {
  relocate_vector <- c(relocate_vector,
                       paste(stratum, c("n_total", "count_mean", "percent_sd"), sep = "_"))
}

#Reorder and Add Statistical Testing
table_infusion_paper <- table_infusion_paper |>
  relocate(all_of(relocate_vector), .after = level) |>
  group_by(variable) |>
  arrange(variable, -level) |>
  mutate(rn=dplyr::row_number()) |>
  left_join(tab_chi, by=c('variable', 'rn')) |>
  arrange(order_number, variable) |>
  select(-order_number, -rn) |>
  filter(level==1) #These are All Binary So Can Do This
```

```{r Now for Continuous Table 1 Generate Weighted Means/SD}
cont_table_function <- function(table_cont) {
  table_cont <- table_cont |>
    group_by(strata, variable) |>
    mutate(
     value_n=n-miss,
     overall_n=sum(value_n, na.rm=T),
     weights = value_n/overall_n,
     weighted_mean=round(sum(mean*weights, na.rm=T), 3),
     weighted_sd=
       round(sqrt(sum((value_n - 1) * sd^2, na.rm = T) / 
                 sum(value_n - 1, na.rm = T)), 2)) |>
      ungroup()
}

table_vent_working <- cont_table_function(table_vent_period) 

#Function for Weighted ANOVA from Summary Data
weighted_anova_pval <- function(table_cont) {
  df <- table_cont |>
    dplyr::group_by(variable) |>
    filter(strata!='Overall') |>
  # Total number of groups
    mutate(k=length(unique(strata))) |>
    #Between-group sum of squares (SSB)
    mutate( 
           # Total sample size across all strata
           overall_n = sum(value_n, na.rm = TRUE),
           # Grand mean across all strata
           grand_mean = sum(mean * value_n, na.rm = TRUE) / sum(value_n, na.rm = TRUE),
           # Between-strata sum of squares
           ss_between = sum(value_n * (mean - grand_mean)^2, na.rm = TRUE),
           # Within-strata sum of squares
           ss_within = sum((value_n - 1) * sd^2, na.rm = TRUE),
           # Degrees of freedom
           df_between = k-1,
           df_within = overall_n-k, 
           # Mean squares
           ms_between = ss_between/df_between,
           ms_within = ss_within/df_within,
           f_stat = round(ms_between/ms_within, 4),
           p.value = round(pf(f_stat, df_between, df_within, lower.tail = FALSE), 4))
  
  new_columns <- df |>
    select(variable, f_stat, p.value) |>
    dplyr::distinct()
  return(new_columns)
}
group_p_values <- weighted_anova_pval(table_vent_working)

table_vent_paper <- table_vent_working |>
  left_join(group_p_values, by='variable') |>
  select(strata, variable, weighted_mean, weighted_sd, f_stat, p.value, overall_n) |>
  distinct() |>
  rename(
    n_total=overall_n,
    count_mean=weighted_mean,
    percent_sd=weighted_sd
  ) |>
  pivot_wider(
    names_from=c(strata),
    names_glue="{strata}_{.value}",
    values_from=c(n_total, count_mean, percent_sd)) 

#Vector For Reordering Columns
strata_values <- c('Overall', 'Pre-COVID', 'COVID', 'Post-COVID')
relocate_vector <- c()
# Loop through the strata values to generate names for those strata
for (stratum in strata_values) {
  relocate_vector <- c(relocate_vector,
                       paste(stratum, c("n_total", "count_mean", "percent_sd"), sep = "_"))
}

#Reorder and Add Statistical Testing
table_vent_paper <- table_vent_paper |>
  relocate(all_of(relocate_vector), .after = variable) |>
  mutate(level=NA_character_) |> #For Combining With Cat Table
  relocate(level, .after=variable)

#Now Table 1 Paper
table_vent_practices <- rowbind(table_vent_paper, table_infusion_paper, fill=T) 

#Make Table 1 Nice for Easy Pasting Into Paper
readable_table <- function(tab, strata_values, has_continuous, has_categorical) {
  
  # First, create summary columns for each stratum
  for (z in strata_values) {
    # Create the new column name
    new_col <- paste0(z, "_summary")
    
    # Create the source column names
    count_col <- paste0(z, "_count_mean")
    percent_col <- paste0(z, "_percent_sd")
    
    # Check if columns exist before trying to use them
    if (count_col %in% names(tab) && percent_col %in% names(tab)) {
      tab <- tab %>%
        mutate(
          !!new_col := case_when(
            # Handle missing values
            is.na(.data[[count_col]]) | is.na(.data[[percent_col]]) ~ NA_character_,
            # Create summary
            TRUE ~ paste0(
              round(.data[[count_col]], digits = 1), 
              ' (', 
              round(.data[[percent_col]], digits = 1), 
              ')'
            )
          )
        )
    }
  }
  
  # Create relocate vector
  relocate_vector <- c()
  for (stratum in strata_values) {
    n_total_col <- paste0(stratum, "_n_total")
    summary_col <- paste0(stratum, "_summary")
    
    # Only add columns that exist
    if (n_total_col %in% names(tab)) {
      relocate_vector <- c(relocate_vector, n_total_col)
    }
    if (summary_col %in% names(tab)) {
      relocate_vector <- c(relocate_vector, summary_col)
    }
  }
  
  # Select columns based on what exists
  base_cols <- c("variable", "level")
  stat_cols <- c()
  
  if ("p.value" %in% names(tab)) stat_cols <- c(stat_cols, "p.value")
  if ("f_stat" %in% names(tab) && has_continuous) stat_cols <- c(stat_cols, "f_stat")
  if ("chisq" %in% names(tab) && has_categorical) stat_cols <- c(stat_cols, "chisq")
  
  # Select only columns that exist
  all_cols <- c(base_cols, relocate_vector, stat_cols)
  existing_cols <- all_cols[all_cols %in% names(tab)]
  
  tab <- tab %>%
    select(all_of(existing_cols))
  
  return(tab)
}

table_vent_practices <- readable_table(table_vent_practices, strata_values, has_continuous=T, has_categorical=T)

#Add Number Missing and Percent Missing
Total_n = N_aggregate$N_aggregate[N_aggregate$strata=='Overall' & N_aggregate$site=='hopkins']
table_vent_practices <- table_vent_practices |>
  mutate(N_aggregate=Total_n,
         n_miss=N_aggregate-Overall_n_total,
         percent_miss = round((n_miss / Overall_n_total) * 100, digits = 2)) |>
  select(-N_aggregate)

#Save Tables
write_csv(table_vent_practices, paste0(project_location, '/summary_output/table_vent_practices.csv'))

#Also Calculate Range of Medians and IQRs for Vent Practices
median_vent_table <- table_vent_period |>
  group_by(strata, variable) %>%
  summarise(
    n_sites = n(),
    total_n = sum(n),
    
    # Median statistics
    median_min = min(median, na.rm = TRUE),
    median_max = max(median, na.rm = TRUE),
    median_range = paste0(round(min(median, na.rm = TRUE), 1), 
                          "-", 
                          round(max(median, na.rm = TRUE), 1)),
    
    # Q1 (25th percentile) statistics
    q1_min = min(p25, na.rm = TRUE),
    q1_max = max(p25, na.rm = TRUE),
    q1_range = paste0(round(min(p25, na.rm = TRUE), 1), 
                      "-", 
                      round(max(p25, na.rm = TRUE), 1)),
    
    # Q3 (75th percentile) statistics
    q3_min = min(p75, na.rm = TRUE),
    q3_max = max(p75, na.rm = TRUE),
    q3_range = paste0(round(min(p75, na.rm = TRUE), 1), 
                      "-", 
                      round(max(p75, na.rm = TRUE), 1)),
    
    # Combined IQR range
    iqr_range = paste0(round(min(p25, na.rm = TRUE), 1), 
                       "-", 
                       round(max(p75, na.rm = TRUE), 1)),
    
    .groups = "drop"
  )

# More readable format with combined median (IQR) column
summary_formatted <- median_vent_table %>%
  mutate(
    median_iqr = paste0(median_range, " (", iqr_range, ")")
  ) %>%
  select(strata, variable, n_sites, total_n, median_range, iqr_range) |>
  mutate(ord=fcase(
    strata=='Overall', 1, 
    strata=='Pre-COVID', 2,
    strata=='COVID', 3),
    ) |>
  left_join(table_vent_paper %>% select(variable) |> mutate(var_ord=row_number())) |>
  arrange(ord, var_ord) |>
  select(-ord, -var_ord)

print(summary_formatted)
write_csv(summary_formatted, paste0(project_location, '/summary_output/table_vent_medians.csv'))
```

```{r Outcomes - Start with Categorical Outcomes - By Proning and Period}
table_cat_outcomes <- collate_tables("_cat_outcomes_by_proned_period.csv") 
table_cont_outcomes <- collate_tables("_cont_outcomes_by_proned_period.csv") 

#Rename Strata
table_cat_outcomes <- table_cat_outcomes |>
  mutate(
    strata = case_when(
      str_ends(strata, "_0") ~ str_replace(strata, "_0$", "_not_prone"),
      str_ends(strata, "_1") ~ str_replace(strata, "_1$", "_proned"),
      TRUE ~ strata  # Keep unchanged if doesn't match pattern
    )
  )


#Now Aggregate Categorical Variables
aggregate_outcomes <- table_cat_outcomes |>
  group_by(strata, variable, level) |>
  dplyr::reframe(
    n_total=sum(n),
    n_miss=sum(miss, na.rm=T),
    count=sum(freq,na.rm=T),
    percent=count/(n_total-n_miss)
  ) 

#Order of Variables
order_vec <- table_cat_outcomes$variable
order <- data.frame(
  variable=unique(order_vec)) |>
  mutate(order_number=dplyr::row_number())

aggregate_outcomes <- aggregate_outcomes |>
  left_join(order, by='variable') |>
  arrange(strata, order_number)

#For now Table 1 Purposes This Ignores Clustering by Site
vars <- unique(aggregate_outcomes$variable) #Can do this as All Variables are Unique

tab_chi <- chisq_function(aggregate_outcomes) |>
  mutate(rn=1)

#Merge Back with Table 1 and Convert to Wide
table_outcomes_cat_paper <- aggregate_outcomes |>
  select(strata, variable, level, n_total, count, percent, order_number) |>
  mutate(
    percent=round((percent*100), digits = 2)
  ) |>
  rename(count_mean=count,
         percent_sd=percent) |>
  pivot_wider(
    names_from=c(strata),
    names_glue="{strata}_{.value}",
    values_from=c(n_total, count_mean, percent_sd)) 

#Vector For Reordering Columns
strata_values <- c('Overall', 
                   'Pre-COVID_proned', 'Pre-COVID_not_prone', 
                   'COVID_proned', 'COVID_not_prone',
                   'Post-COVID_proned', 'Post-COVID_not_prone')
relocate_vector <- c()
# Loop through the strata values to generate names for those strata
for (stratum in strata_values) {
  relocate_vector <- c(relocate_vector,
                       paste(stratum, c("n_total", "count_mean", "percent_sd"), sep = "_"))
}

#Reorder and Add Statistical Testing
table_outcomes_cat_paper <- table_outcomes_cat_paper |>
  relocate(all_of(relocate_vector), .after = level) |>
  group_by(variable) |>
  arrange(variable, -level) |>
  mutate(rn=dplyr::row_number()) |>
  left_join(tab_chi, by=c('variable', 'rn')) |>
  arrange(order_number, variable) |>
  select(-order_number, -rn) |>
  filter(level==1) #These are All Binary So Can Do This
```

```{r Continuous Outcomes Table}
table_outcomes_cont_working <- cont_table_function(table_cont_outcomes) |>
  mutate(
    strata = case_when(
      str_ends(strata, "_0") ~ str_replace(strata, "_0$", "_not_prone"),
      str_ends(strata, "_1") ~ str_replace(strata, "_1$", "_proned"),
      TRUE ~ strata  # Keep unchanged if doesn't match pattern
    )
  )
group_p_values <- weighted_anova_pval(table_outcomes_cont_working)

table_outcomes_cont_working <- table_outcomes_cont_working |>
  left_join(group_p_values, by='variable') |>
  select(strata, variable, weighted_mean, weighted_sd, f_stat, p.value, overall_n) |>
  distinct() |>
  rename(
    n_total=overall_n,
    count_mean=weighted_mean,
    percent_sd=weighted_sd
  ) |>
  pivot_wider(
    names_from=c(strata),
    names_glue="{strata}_{.value}",
    values_from=c(n_total, count_mean, percent_sd)) 

#Vector For Reordering Columns
strata_values <- c('Overall', 
                   'Pre-COVID_proned', 'Pre-COVID_not_prone', 
                   'COVID_proned', 'COVID_not_prone',
                   'Post-COVID_proned', 'Post-COVID_not_prone')
relocate_vector <- c()
# Loop through the strata values to generate names for those strata
for (stratum in strata_values) {
  relocate_vector <- c(relocate_vector,
                       paste(stratum, c("n_total", "count_mean", "percent_sd"), sep = "_"))
}

#Reorder and Add Statistical Testing
table_outcomes_paper <- table_outcomes_cont_working |>
  relocate(all_of(relocate_vector), .after = variable) |>
  mutate(level=NA_character_) |> #For Combining With Cat Table
  relocate(level, .after=variable)

#Now Table 1 Paper
table_outcomes_paper <- rowbind(table_outcomes_cat_paper, table_outcomes_paper, fill=T) |>
  ungroup()
```

```{r Outcomes Table for Easy-ier Reading}
table_outcomes_paper <- readable_table(table_outcomes_paper, strata_values, has_continuous=T, has_categorical=T)

#Save Tables
write_csv(table_outcomes_paper, paste0(project_location, '/summary_output/table_outcomes_proned_period.csv'))

#Also Calculate Range of Medians and IQRs for Outcomes
median_outcomes_table <- table_cont_outcomes |>
  group_by(strata, variable) %>%
  summarise(
    n_sites = n(),
    total_n = sum(n),
    
    # Median statistics
    median_min = min(median, na.rm = TRUE),
    median_max = max(median, na.rm = TRUE),
    median_range = paste0(round(min(median, na.rm = TRUE), 1), 
                          "-", 
                          round(max(median, na.rm = TRUE), 1)),
    
    # Q1 (25th percentile) statistics
    q1_min = min(p25, na.rm = TRUE),
    q1_max = max(p25, na.rm = TRUE),
    q1_range = paste0(round(min(p25, na.rm = TRUE), 1), 
                      "-", 
                      round(max(p25, na.rm = TRUE), 1)),
    
    # Q3 (75th percentile) statistics
    q3_min = min(p75, na.rm = TRUE),
    q3_max = max(p75, na.rm = TRUE),
    q3_range = paste0(round(min(p75, na.rm = TRUE), 1), 
                      "-", 
                      round(max(p75, na.rm = TRUE), 1)),
    
    # Combined IQR range
    iqr_range = paste0(round(min(p25, na.rm = TRUE), 1), 
                       "-", 
                       round(max(p75, na.rm = TRUE), 1)),
    
    .groups = "drop"
  )

# More readable format with combined median (IQR) column
summary_formatted <- median_outcomes_table %>%
  mutate(
    median_iqr = paste0(median_range, " (", iqr_range, ")")
  ) %>%
  select(strata, variable, n_sites, total_n, median_range, iqr_range) |>
  mutate(ord=fcase(
    strata=='Overall', 1, 
    strata=='Pre-COVID_proned', 2,
    strata=='Pre-COVID_not_prone', 3,
    strata=='COVID_proned', 2,
    strata=='COVID_not_prone', 3,
     strata=='Post-COVID_proned', 2,
    strata=='Post-COVID_not_prone', 3)) |>
  left_join(table_vent_paper %>% select(variable) |> mutate(var_ord=row_number())) |>
  arrange(ord, var_ord) |>
  select(-ord, -var_ord)

print(summary_formatted)
write_csv(summary_formatted, paste0(project_location, '/summary_output/table_outcomes_medians.csv'))
```

